<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Project 3 — Stitching Photo Mosaics</title>
  <style>
    :root { --maxw: 980px; }
    body {
      font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif;
      margin: 0; line-height: 1.6; color: #222; background: #fafafa;
    }
    header, main, footer { max-width: var(--maxw); margin: 0 auto; padding: 20px; }
    header h1 { margin: 0 0 6px; }
    header a { color: #0b5fff; text-decoration: none; }
    section {
      background: #fff; border: 1px solid #eee; border-radius: 14px;
      padding: 18px; margin: 18px 0 24px; box-shadow: 0 1px 3px rgba(0,0,0,.05);
    }
    h2 { margin: 0 0 12px; font-size: 1.25rem; }
    h3 { margin: 8px 0 8px; font-size: 1.1rem; }
    p { margin: 8px 0; }
    pre { background:#0f172a; color:#e5e7eb; padding:14px; border-radius:10px; overflow:auto; }
    code { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", monospace; }
    .grid-2 { display:grid; grid-template-columns:repeat(2,minmax(0,1fr)); gap:12px; }
    .grid-4 { display:grid; grid-template-columns:repeat(4,minmax(0,1fr)); gap:12px; }
    @media (max-width: 900px) { .grid-4 { grid-template-columns:repeat(2,minmax(0,1fr)); } }
    @media (max-width: 720px) { .grid-2 { grid-template-columns:1fr; } }
    figure { margin:0; border:1px solid #eee; border-radius:12px; overflow:hidden; background:#fff; }
    figure img, figure video { display:block; width:100%; height:auto; }
    figcaption { font-size:.9rem; padding:10px 12px; background:#f6f7fb; border-top:1px solid #eee; }
    .answer { background:#f4fbf6; border:1px solid #d8f1df; border-radius:10px; padding:10px 12px; margin-top:10px; }
    .note { font-size:.92rem; color:#555; background:#fff7e6; border:1px solid #ffe2a8; border-radius:10px; padding:10px 12px; margin-top:10px; }
    footer { font-size:.9rem; color:#666; }
    .back { display:inline-block; margin-top:6px; }
    ul.tight { margin:8px 0; padding-left: 18px; }
    .grid-3 { display:grid; grid-template-columns:repeat(3,minmax(0,1fr)); gap:12px; }
  </style>
  <!-- MathJax for nicely typeset matrices/equations -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <header>
    <h1>Project 3 — Stitching Photo Mosaics</h1>
    <a class="back" href="../index.html">← Back to main portfolio</a>
  </header>

  <main>
    <section id="part-a">
      <h2>Part A: IMAGE WARPING and MOSAICING</h2>
      <section id="a1">
      <h3>A.1: Shoot the Pictures</h3>
        <p>
          Images can be related by many transformations. In this part we care about the
          <strong>perspective/projective</strong> case—the camera model that explains how a planar scene looks from
          slightly different viewpoints. You can think of translation/rotation/affine as simpler limits, while
          projective transforms cover the full perspective effects we see in photos.
        </p>
        <p>
          Intuitively, a projective warp remaps the image as if the camera were tilted or the plane were viewed from a
          new angle. A few useful facts:
        </p>
        <ul class="tight">
        <li>Straight lines remain straight, but parallel lines may meet at a vanishing point.</li>
        <li>Distances and their ratios are generally <em>not</em> preserved.</li>
        <li>There is no requirement that the origin maps to the origin.</li>
        <li>Composing two projective warps yields another projective warp.</li>
        <li>Algebraically, it is a change of coordinates in projective space using a 3×3 homography.</li>
        </ul>
        <p>
          Below are two example pairs shot on campus. For each pair, the <em>center of projection</em> is held fixed while the
          camera rotates, producing the desired projective relationship that we will use later for rectification and
          mosaicing.
        </p>

      <!-- Set 1 placeholders -->
      <div class="grid-2">
        <figure>
          <img src="media/WechatIMG115.jpg" alt="Scene 1 from a straight angle" loading="lazy">
          <figcaption>Scene 1 — Figure 1</figcaption>
        </figure>
        <figure>
          <img src="media/WechatIMG114.jpg" alt="Scene 1 from a rotated angle" loading="lazy">
          <figcaption>Scene 1 — Figure 2</figcaption>
        </figure>
      </div>

      <!-- Set 2 placeholders -->
      <div class="grid-2" style="margin-top:12px;">
        <figure>
          <img src="media/WechatIMG119.jpg" alt="Scene 2 from a straight angle" loading="lazy">
          <figcaption>Scene 2 — Figure 2</figcaption>
        </figure>
        <figure>
          <img src="media/WechatIMG118.jpg" alt="Scene 2 from a rotated angle" loading="lazy">
          <figcaption>Scene 2 — Figure 2</figcaption>
        </figure>
      </div>
      </section>

      <section id="a2">
        <h2>A.2: Recover Homographies</h2>
      
        <!-- Two original images -->
        <div class="grid-2">
          <figure>
            <img src="media/WechatIMG115.jpg" alt="Image 1 (source)" />
            <figcaption>Original Image 1</figcaption>
          </figure>
          <figure>
            <img src="media/WechatIMG114.jpg" alt="Image 2 (target)" />
            <figcaption>Original Image 2</figcaption>
          </figure>
        </div>
      
        <!-- Correspondences visualization -->
        <figure style="margin-top:12px;">
          <img src="media/a2preview.jpg" alt="Point correspondences visualized side-by-side" />
          <figcaption>Eight hand-picked correspondences visualized on the image pair.</figcaption>
        </figure>
      
        <!-- Explanation + equations (MathJax) -->
        <div class="note" style="margin-top:14px;">
          We estimate a projective warp (homography) \(H\) such that each correspondence
          \((x_i, y_i) \leftrightarrow (u_i, v_i)\) satisfies
          \[
            \begin{bmatrix}\lambda u_i\\ \lambda v_i\\ \lambda\end{bmatrix}
            =
            \begin{bmatrix}
              h_1 & h_2 & h_3\\
              h_4 & h_5 & h_6\\
              h_7 & h_8 & 1
            \end{bmatrix}
            \begin{bmatrix}x_i\\ y_i\\ 1\end{bmatrix}.
          \]
          Eliminating \(\lambda\) yields two linear equations per match. Stacking all
          \(n\) matches (here \(n=8\)) gives an over-determined linear system
          \(A\mathbf{h}=\mathbf{b}\) for the unknown vector
          \(\mathbf{h}=[h_1,h_2,h_3,h_4,h_5,h_6,h_7,h_8]^T\):
          \[
          \underbrace{
            \begin{bmatrix}
              x_1 & y_1 & 1 & 0   & 0   & 0 & -u_1x_1 & -u_1y_1\\
              0   & 0   & 0 & x_1 & y_1 & 1 & -v_1x_1 & -v_1y_1\\
              \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
              x_n & y_n & 1 & 0   & 0   & 0 & -u_nx_n & -u_ny_n\\
              0   & 0   & 0 & x_n & y_n & 1 & -v_nx_n & -v_ny_n
            \end{bmatrix}
          }_{A \in \mathbb{R}^{2n\times 8}}
          \;
          \underbrace{
            \begin{bmatrix}
              h_1\\h_2\\h_3\\h_4\\h_5\\h_6\\h_7\\h_8
            \end{bmatrix}
          }_{\mathbf{h}}
          =
          \underbrace{
            \begin{bmatrix}
              u_1\\ v_1\\ \vdots\\ u_n\\ v_n
            \end{bmatrix}
          }_{\mathbf{b}}.
          \]
          We solve for \(\hat{\mathbf{h}}\) in the least-squares sense and assemble
          \[
            H=
            \begin{bmatrix}
              h_1 & h_2 & h_3\\
              h_4 & h_5 & h_6\\
              h_7 & h_8 & 1
            \end{bmatrix},
          \]
          followed by a scale normalization so that \(H_{33}=1\).
        </div>
      
        <!-- How correspondences were collected (math, not code) -->
        <div class="answer" style="margin-top:14px;">
          We collected eight correspondences using the
          <a href="https://cal-cs180.github.io/fa23/hw/proj3/tool.html" target="_blank" rel="noopener">online point-clicking tool</a>.
          The selected coordinates are shown below (rows correspond to matches):
          \[
            \text{Image 1 (source) } \{(x_i,y_i)\}_{i=1}^{8} =
            \begin{bmatrix}
            666 & 2455\\
            1178 & 1768\\
            2307 & 1098\\
            1036 & 1526\\
            1716 & 1926\\
            2083 & 4048\\
            1539 & 3011\\
            2205 & 1785
            \end{bmatrix},\qquad
            \text{Image 2 (target) } \{(u_i,v_i)\}_{i=1}^{8} =
            \begin{bmatrix}
            2531 & 2415\\
            2984 & 1674\\
            4233 &  696\\
            2837 & 1465\\
            3559 & 1736\\
            4105 & 4044\\
            3407 & 2910\\
            4141 & 1486
            \end{bmatrix}.
          \]
          Substituting these into the system above and solving in least squares produces the homography below.
        </div>
      
        <!-- Recovered H (MathJax numeric matrix) -->
        <h3 style="margin-top:14px;">Recovered Homography \(H\)</h3>
        \[
        H=
        \begin{bmatrix}
        5.88791765\times10^{-1} & 1.63905192\times10^{-2} & 1.89669153\times10^{3}\\
        -3.18000604\times10^{-1} & 8.60996283\times10^{-1} & 3.23268751\times10^{2}\\
        -9.53829001\times10^{-5} & -5.88331546\times10^{-6} & 1
        \end{bmatrix}
        \]
      </section>

      <section id="a3">
        <h2>A.3: Warp the Images</h2>
      
        <p>
          We implement two image warping routines using <em>inverse warping</em> and custom interpolation:
          <strong>Nearest Neighbor Interpolation</strong> and <strong>Bilinear Interpolation</strong>.
          Inverse warping treats each output pixel \((x_o,y_o)\) as a query: we map it back with
          \(H^{-1}\) to a (generally non-integer) input location \((x_i,y_i)\). Sampling at
          \((x_i,y_i)\) gives the color of \((x_o,y_o)\). This avoids holes that would appear with
          forward mapping. We also track a per-pixel <strong>alpha mask</strong> that marks whether the
          back-projected location fell inside the source image (useful for visualization and later blending).
        </p>
      
        <h3>Nearest Neighbor Interpolation</h3>
        <p>
          For each back-projected location \((x_i,y_i)\), we round to the closest integer pixel
          \((\mathrm{round}(x_i), \mathrm{round}(y_i))\) and copy that value. This method is very fast and
          preserves sharp edges, but can introduce aliasing and blocky artifacts when the image is rotated
          or scaled.
        </p>
      
        <h3>Bilinear Interpolation</h3>
        <p>
          For each \((x_i,y_i)\), we use the four neighbors \((x_0,y_0),(x_1,y_0),(x_0,y_1),(x_1,y_1)\)
          (with \(x_0=\lfloor x_i\rfloor, x_1=x_0+1, y_0=\lfloor y_i\rfloor, y_1=y_0+1\)) and blend them with
          weights based on the fractional offsets \(w_x=x_i-x_0,\, w_y=y_i-y_0\). This reduces jaggies and
          yields smoother results, at the cost of slight blur and a bit more compute.
        </p>
      
        <h3>Images for Rectification</h3>
        <p>
          We rectify planar objects that are known to be square but appear as general quadrilaterals due to
          perspective. For each case, we (1) click the four corners on the source image (the object in the
          photo) and (2) define target corners as a square. We then compute \(H\) from these two
          4-point sets and warp the source with both interpolations.
        </p>
      
        <!-- Row 1: Painting -->
        <h4 style="margin-top:10px;">Set 1 — Painting</h4>
        <div class="grid-3">
          <figure>
            <img src="media/source2.jpg" alt="Painting (original)" />
            <figcaption>Original</figcaption>
          </figure>
          <figure>
            <img src="media/rectified_nearest_painting.png" alt="Painting rectified (nearest)" />
            <figcaption>Rectified — Nearest</figcaption>
          </figure>
          <figure>
            <img src="media/rectified_bilinear_painting.png" alt="Painting rectified (bilinear)" />
            <figcaption>Rectified — Bilinear</figcaption>
          </figure>
        </div>
      
        <!-- Row 2: Floor -->
        <h4 style="margin-top:10px;">Set 2 — Floor</h4>
        <div class="grid-3">
          <figure>
            <img src="media/source5.jpg" alt="Floor (original)" />
            <figcaption>Original</figcaption>
          </figure>
          <figure>
            <img src="media/rectified_nearest_floor.png" alt="Floor rectified (nearest)" />
            <figcaption>Rectified — Nearest</figcaption>
          </figure>
          <figure>
            <img src="media/rectified_bilinear_floor.png" alt="Floor rectified (bilinear)" />
            <figcaption>Rectified — Bilinear</figcaption>
          </figure>
        </div>
      
        <h3>How we compute \(H\) for rectification</h3>
        <p>
          Because the selected regions are squares in the world, the four clicked corners on the original
          image are our source points \(\{(x_i,y_i)\}_{i=1}^4\). We hand-define the target as a perfect
          rectangle, e.g. \(\{(0,0),(W,0),(W,H),(0,H)\}\). Using these two sets, we recover the homography
          \(H\) and then apply inverse warping with the two interpolation schemes
          to obtain the rectified results shown above.
        </p>
      
        <h3>Nearest vs. Bilinear: brief comparison</h3>
        <ul class="tight">
          <li><strong>Quality:</strong> Bilinear reduces aliasing and produces smoother, more natural rectifications; Nearest shows staircase artifacts on edges. 
                                        This can be clearly observed in the comparison of the two sets of rectified images above, and this phenomenon is particularly noticeable along the edges of walls (painting image)and floor tiles (floor image).</li>
          <li><strong>Sharpness:</strong> Nearest preserves edge crispness but looks blocky; Bilinear is slightly softer.</li>
          <li><strong>Speed:</strong> Nearest is fastest; Bilinear is modestly slower but still lightweight for our image sizes.</li>
        </ul>
      </section>

      <section id="a4">
        <h2>A.4: Blend the Images into a Mosaic</h2>
      
        <p>
          This mosaicing pipeline has three stages: <strong>(1) register</strong> images with a homography, <strong>(2) warp</strong> all images
          into a shared canvas using inverse warping, and <strong>(3) <em>blend</em></strong> the aligned layers to hide seams.
          Concretely, we pick one image as the reference frame (or define a new canvas), compute the mapping
          from each other image into that frame, and predict the global canvas bounds by transforming the four
          corners. We then render each source into the canvas with inverse mapping (sampling from the source
          using nearest/bilinear interpolation) and track an <em>alpha mask</em> that marks valid pixels.
        </p>
      
        <p>
          To reduce edge artifacts we combine several lightweight steps:
        </p>
        <ul class="tight">
          <li><strong>Gain matching in the overlap:</strong> scale the second image to match the first image’s brightness in regions where both are valid.</li>
          <li><strong>Feathered weights:</strong> build per-image alpha that is high near the image center and smoothly decays to zero at the borders (distance-to-edge), so seams are softly shared.</li>
        </ul>
      
        <p class="note">
          Practical notes: (i) We always blend images that are already aligned in the same canvas and size; (ii) weights are
          normalized at each pixel to avoid exposure jumps.
        </p>
      
        <!-- ===== Result Set 1 ===== -->
        <h3 style="margin-top: 10px;">Mosaic Set 1</h3>
        <div class="grid-2">
          <figure>
            <img src="media/WechatIMG114.jpg" alt="Set 1 — Source image A" />
            <figcaption>Set&nbsp;1 — Image 1 (reference)</figcaption>
          </figure>
          <figure>
            <img src="media/WechatIMG115.jpg" alt="Set 1 — Source image B" />
            <figcaption>Set&nbsp;1 — Image 2 (warped to reference)</figcaption>
          </figure>
        </div>
        <figure style="margin-top:10px;">
          <img src="media/4_1.png" alt="Set 1 — Blended mosaic" />
          <figcaption>Set&nbsp;1 — Blended mosaic</figcaption>
        </figure>
      
        <!-- ===== Result Set 2 ===== -->
        <h3 style="margin-top: 18px;">Mosaic Set 2</h3>
        <div class="grid-2">
          <figure>
            <img src="media/WechatIMG121.jpg" alt="Set 2 — Source image A" />
            <figcaption>Set&nbsp;2 — Image 1 (reference)</figcaption>
          </figure>
          <figure>
            <img src="media/WechatIMG120.jpg" alt="Set 2 — Source image B" />
            <figcaption>Set&nbsp;2 — Image 2 (warped to reference)</figcaption>
          </figure>
        </div>
        <figure style="margin-top:10px;">
          <img src="media/4_2.png" alt="Set 2 — Blended mosaic" />
          <figcaption>Set&nbsp;2 — Blended mosaic</figcaption>
        </figure>
      
        <!-- ===== Result Set 3 ===== -->
        <h3 style="margin-top: 18px;">Mosaic Set 3</h3>
        <div class="grid-2">
          <figure>
            <img src="media/WechatIMG113.jpg" alt="Set 3 — Source image A" />
            <figcaption>Set&nbsp;3 — Image 1 (reference)</figcaption>
          </figure>
          <figure>
            <img src="media/WechatIMG112.jpg" alt="Set 3 — Source image B" />
            <figcaption>Set&nbsp;3 — Image 2 (warped to reference)</figcaption>
          </figure>
        </div>
        <figure style="margin-top:10px;">
          <img src="media/4_3.png" alt="Set 3 — Blended mosaic" />
          <figcaption>Set&nbsp;3 — Blended mosaic</figcaption>
        </figure>
      </section>
    </section>

    <!-- ======================= Part B ======================= -->
    <section id="part-b">
      <h2>Part B — FEATURE MATCHING for AUTOSTITCHING</h2>
    
      <section id="b1">
        <h3>B.1: Harris Corner Detection</h3>
    
        <p>
          We begin the automatic pipeline by detecting repeatable 2D features. Our detector is the
          classic <strong>Harris corner</strong>, followed by <strong>Adaptive Non-Maximal Suppression</strong> (ANMS) to thin out clusters and
          distribute features spatially. At a single scale, Harris computes a structure-tensor
          response \(R\); we then keep local maxima of \(R\)
          and finally apply ANMS so that retained points are strong <em>and</em> well spread across the image.
        </p>
    
        <p>
          <strong>Why ANMS?</strong> Plain Harris often returns many neighboring peaks along edges, windows, or textures.
          ANMS enforces a <em>distance to stronger neighbors</em>, so we keep fewer, stronger, and more
          <em>uniformly distributed</em> features, which improves matching robustness and RANSAC later.
          To implement Adaptive Non-Maximal Suppression (ANMS), I computed a suppression radius for each
          candidate corner point as described in the paper <em>"Multi-Image Matching using Multi-Scale Oriented Patches"</em>.
          The suppression radius is defined as the minimum distance to another corner that is stronger,
          scaled by a factor of <code>c = 0.9</code>. By selecting corners with the largest suppression radii,
          we obtain a well-distributed set of feature points across the image. From this, I kept the
          top <code>600</code> corners as the final interest points.
        </p>
    
        <!-- Scene 1 -->
        <h3 style="margin-top:14px; font-size:1.05rem;">Image 1</h3>
        <div class="grid-2">
          <figure>
            <img src="media/WechatIMG114.jpg_corners_harris_top6000.png" alt="Harris corners before ANMS — scene 1">
            <figcaption>Before ANMS — Harris</figcaption>
          </figure>
          <figure>
            <img src="media/WechatIMG114.jpg_corners_anms_top600.png" alt="Corners after ANMS — scene 1">
            <figcaption>After ANMS — \(c=0.9\), keep=600</figcaption>
          </figure>
        </div>
    
        <!-- Scene 2 (placeholders: replace with your actual files) -->
        <h3 style="margin-top:10px; font-size:1.05rem;">Image 2</h3>
        <div class="grid-2">
          <figure>
            <img src="media/WechatIMG115.jpg_corners_harris_top6000.png" alt="Harris corners before ANMS — scene 2">
            <figcaption>Before ANMS — Harris</figcaption>
          </figure>
          <figure>
            <img src="media/WechatIMG115.jpg_corners_anms_top600.png" alt="Corners after ANMS — scene 2">
            <figcaption>After ANMS — \(c=0.9\), keep=600</figcaption>
          </figure>
        </div>
      </section>

      <section id="b2">
        <h3>B.2: Feature Descriptor Extraction</h3>

        <p>
          For each interest point from B.1, we represent local appearance with a compact,
          illumination-robust patch descriptor. Following the spirit of Brown et&nbsp;al. (MOPS),
          we extract an <em>axis-aligned</em> 8×8 intensity patch from a larger 40×40 window
          centered at the keypoint (no rotation or multi-scale in this assignment).
          The 40×40 window is first low-pass filtered (Gaussian) and then uniformly
          resampled onto an 8×8 grid (5-pixel spacing, bilinear sampling). Sampling from a
          blurred, larger window gives a smooth, <em>low-frequency signature</em> of the neighborhood,
          which is far more stable to pixel-level misalignment and sensor noise than a raw 8×8 crop.
        </p>

        <p>
          Finally, each 8×8 patch is <strong>bias/gain normalized</strong> (zero-mean, unit-variance).
          This removes brightness and contrast changes (linear photometric variations) and puts all
          descriptors on a comparable scale, so that simple SSD behaves similarly to normalized
          correlation. In short, the pipeline is:
        </p>

        <ul>
          <li>Center a 40×40 window at each ANMS keypoint (reflect padding near borders).</li>
          <li>Apply Gaussian blur to prevent aliasing.</li>
          <li>Resample the blurred window to 8×8 at cell centers (5-pixel stride) with bilinear interpolation.</li>
          <li>Bias/gain normalization: subtract mean and divide by standard deviation.</li>
        </ul>

        <p>
          The result is a 64-D descriptor that captures the <em>coarse structure</em> of the neighborhood
          and is robust to small geometric jitter and linear illumination change. For visualization,
          we show the top-80 normalized 8×8 patches (ranked by gradient energy) from each image.
        </p>

        <div class="grid-2">
          <figure>
            <img src="media/114.png" alt="Top-80 normalized 8×8 descriptors for image 1">
            <figcaption>Image&nbsp;1 — Top-80 normalized 8×8 descriptors (sampled from blurred 40×40 windows).</figcaption>
          </figure>
          <figure>
            <img src="media/115.png" alt="Top-80 normalized 8×8 descriptors for image 2">
            <figcaption>Image&nbsp;2 — Top-80 normalized 8×8 descriptors (sampled from blurred 40×40 windows).</figcaption>
          </figure>
        </div>
      </section>

      <section id="b3">
        <h3>B.3: Feature Matching</h3>

        <p>
          Given the ANMS keypoints and the normalized 8×8 descriptors from B.2, we match features
          across images using a nearest-neighbor strategy that balances recall and precision.
          Similarity is measured with SSD on the normalized patches (equivalently, a monotonic
          transform of normalized correlation). To reduce ambiguous matches on repeated textures,
          we adopt Lowe’s <em>ratio test</em>: for each descriptor in image&nbsp;1 we find its two nearest
          neighbors in image&nbsp;2 with distances \(d_1\) and \(d_2\); we accept the match only if
          \(d_1/d_2 < \tau\), meaning the best match is
          significantly better than the second best. We then apply a <em>mutual consistency</em>
          check (cross-check): a match is kept only if the chosen pair is also each other’s nearest
          neighbor in the reverse direction. This removes many one-sided coincidences.
        </p>

        <p>
          For visualization, we provide two complementary views. The <strong>clear view</strong> shows a small,
          spatially dispersed subset of the strongest correspondences (selected by the best ratio
          and simple spatial non-max suppression) with thick, colored lines and labels—easy to audit.
          The <strong>full view</strong> draws a large set of accepted matches with thin, semi-transparent lines,
          revealing the global structure and coverage. To further sanity-check the appearance
          similarity, we include a <strong>patch comparison panel</strong> that, for a handful of matched pairs,
          displays the 40×40 blurred source windows and the resulting 8×8 normalized descriptors
          (upsampled for visibility). Good pairs exhibit closely matching low-frequency structure.
        </p>

        <!-- Clear vs. Full -->
        <div class="grid-2">
          <figure>
            <img src="media/matches_clear_FIXED_WechatIMG114_vs_WechatIMG115.png"
                 alt="Clean, spatially thinned feature matches">
            <figcaption>Clean view — top-ranked, spatially thinned matches (thick colored lines).</figcaption>
          </figure>
          <figure>
            <img src="media/matches_full_FIXED_WechatIMG114_vs_WechatIMG115.png"
                 alt="Dense visualization of all accepted matches">
            <figcaption>Full view — dense visualization of accepted matches (semi-transparent thin lines).</figcaption>
          </figure>
        </div>

        <!-- Patch pair panel -->
        <figure style="margin-top:12px;">
          <img src="media/patch_panel_WechatIMG114_vs_WechatIMG115_K40.png"
               alt="Patch comparison panel showing 40×40 blurred patches and 8×8 normalized descriptors for matched pairs">
          <figcaption>
            Patch comparison panel — for selected matches, each tile shows the 40×40 blurred image
            patches (top row) and the corresponding 8×8 normalized descriptors (bottom row), one pair
            per column. Ratios are annotated for quick quality inspection.
          </figcaption>
        </figure>
      </section>

      
      <section id="b4">
        <h3>B.4: RANSAC for Robust Homography</h3>

        <p>
          To turn pairwise feature matches into a reliable image alignment, we estimate a single
          projective mapping (homography) between the two views using a <strong>4-point RANSAC</strong> loop.
          At each iteration we randomly sample four correspondences, compute a candidate homography
          with normalized DLT, and score it by the <em>reprojection error</em> of all matches in the target
          image. Inliers are those whose error falls below a pixel threshold. We keep the model that
          produces the largest inlier set and then <em>re-estimate</em> the homography from all inliers to
          obtain the final transformation. This procedure is robust to outliers from mismatches,
          repeated textures, or small non-planar effects.
        </p>

        <ul>
          <li>Minimal sample: 4 correspondences \((\mathbf{x}_2 \leftrightarrow \mathbf{x}_1)\); solve \( \mathbf{x}_1 \sim \mathbf{H}\,\mathbf{x}_2 \) via normalized DLT.</li>
          <li>Score by reprojection: \( \|\hat{\mathbf{x}}_1 - \mathbf{x}_1\| \); inliers if error &lt; \(\tau\) px.</li>
          <li>Iterate until confidence is met; refine \(\mathbf{H}\) with all inliers.</li>
          <li>Mosaicing: project image-2 corners to define the canvas, <em>inverse-warp</em> image-2 into image-1’s frame, then blend in the overlap with smooth, seam-aware weights.</li>
        </ul>

        <p>
          Below we compare <strong>manual stitching</strong> (homographies from hand-picked correspondences)
          with our <strong>automatic</strong> pipeline (Harris+ANMS → 8×8 descriptors → ratio-test matching → 4-point RANSAC).
        </p>

        <!-- Mosaic Pair 1 -->
        <h4 style="margin-top:14px;">Mosaic Pair 1</h4>
        <div class="grid-2">
          <figure>
            <img src="media/4_1.png" alt="Manual mosaic — pair 1">
            <figcaption>Manual stitching (pair 1)</figcaption>
          </figure>
          <figure>
            <img src="media/mosaic_auto_WechatIMG114_vs_WechatIMG115.png" alt="Automatic mosaic — pair 1">
            <figcaption>Automatic stitching (pair 1)</figcaption>
          </figure>
        </div>

        <!-- Mosaic Pair 2 -->
        <h4 style="margin-top:12px;">Mosaic Pair 2</h4>
        <div class="grid-2">
          <figure>
            <img src="media/4_2.png" alt="Manual mosaic — pair 2">
            <figcaption>Manual stitching (pair 2)</figcaption>
          </figure>
          <figure>
            <img src="media/mosaic_auto_WechatIMG121_vs_WechatIMG120.png" alt="Automatic mosaic — pair 2">
            <figcaption>Automatic stitching (pair 2)</figcaption>
          </figure>
        </div>

        <!-- Mosaic Pair 3 -->
        <h4 style="margin-top:12px;">Mosaic Pair 3</h4>
        <div class="grid-2">
          <figure>
            <img src="media/4_3.png" alt="Manual mosaic — pair 3">
            <figcaption>Manual stitching (pair 3)</figcaption>
          </figure>
          <figure>
            <img src="media/mosaic_auto_WechatIMG113_vs_WechatIMG112.png" alt="Automatic mosaic — pair 3">
            <figcaption>Automatic stitching (pair 3)</figcaption>
          </figure>
        </div>

        <p style="margin-top:14px;">
          <strong>Manual vs. Automatic.</strong> Manual stitching can be very accurate when high-quality,
          well-spread correspondences are chosen, but it is time-consuming and does not scale. Also, due to 
          the precision errors inherent in manual selection, we can clearly see from the above examples that 
          automatic stitching results effectively reduce the blurring present in manual stitching results. Besides, the
          automatic pipeline is <em>repeatable</em> and <em>scalable</em>: it finds many candidate matches and uses
          RANSAC to discard outliers before refining a single homography, then warps and blends the
          images into a seamless mosaic. Automatic results may degrade on very low-texture scenes,
          strong parallax/non-planarity, or large viewpoint changes, but with sufficient inliers
          they closely match manual quality while requiring no user input. 
        </p>
      </section>

      
    </section>


  </main>

  <footer>
    © <script>document.write(new Date().getFullYear());</script> Project 3. All media courtesy of the author.
  </footer>
</body>
</html>

