<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Project 4 — Neural Radiance Field (NeRF)</title>
  <style>
    :root { --maxw: 980px; }
    body {
      font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif;
      margin: 0; line-height: 1.6; color: #222; background: #fafafa;
    }
    header, main, footer { max-width: var(--maxw); margin: 0 auto; padding: 20px; }
    header h1 { margin: 0 0 6px; }
    header a { color: #0b5fff; text-decoration: none; }
    .back { display: inline-block; margin-top: 6px; }
    section {
      background: #fff; border: 1px solid #eee; border-radius: 14px;
      padding: 18px; margin: 18px 0 24px; box-shadow: 0 1px 3px rgba(0,0,0,.05);
    }
    h2 { margin: 0 0 12px; font-size: 1.25rem; }
    p { margin: 8px 0; }
    .grid-2 { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 12px; }
    @media (max-width: 720px) { .grid-2 { grid-template-columns: 1fr; } }

    figure { margin: 0; border: 1px solid #eee; border-radius: 12px; overflow: hidden; background: #fff; }
    figure img, figure video { display: block; width: 100%; height: auto; }
    figcaption { font-size: .9rem; padding: 10px 12px; background: #f6f7fb; border-top: 1px solid #eee; }

    .answer { background: #f4fbf6; border: 1px solid #d8f1df; border-radius: 10px; padding: 10px 12px; margin-top: 10px; }
    .note { font-size: .92rem; color: #555; background: #fff7e6; border: 1px solid #ffe2a8; border-radius: 10px; padding: 10px 12px; margin-top: 10px; }

    /* Tables */
    table { width: 100%; border-collapse: collapse; }
    thead th { text-align: left; font-weight: 600; padding: 10px 10px; background: #f6f7fb; border-bottom: 1px solid #eee; }
    tbody td { padding: 10px; border-top: 1px solid #eee; vertical-align: middle; }
    td img { width: 240px; height: auto; display: block; border-radius: 10px; border: 1px solid #eee; }
    td.number, td.time { white-space: nowrap; font-variant-numeric: tabular-nums; }
    @media (max-width: 720px) { td img { width: 100%; } }

    footer { font-size: .9rem; color: #666; }
  </style>
</head>
<body>
  <header>
    <h1>Project 4 — Neural Radiance Field (NeRF)</h1>
    <a class="back" href="../index.html">← Back to main portfolio</a>
  </header>

  <main>

    <!-- Part 0 title only -->
    <section id="part0">
      <h2>Part 0 — Calibrating Your Camera, Capturing a 3D Scan, Estimating Poses, and Building the Dataset</h2>
      <!-- Content to be filled in next iteration: 0.1 Calibration, 0.2 Capture, 0.3 Pose (Viser frustums), 0.4 Undistort & NPZ packaging -->
      <!-- Part 0.1 -->
      <section id="part01">
      <h2>Part 0.1 — Calibrating Your Camera</h2>
      <p>
        I calibrate the phone camera using a printed 3×2 grid of ArUco tags (DICT_4X4_50), sized 
        <strong>60&nbsp;mm</strong> square each, with center-to-center spacing of <strong>90.00&nbsp;mm (H)</strong> and <strong>75.67&nbsp;mm (V)</strong>.
        I captured <strong>30–50 images</strong> at a fixed zoom and resolution, while varying viewing angle and distance to
        improve numerical conditioning. Blurry frames or images with mismatched resolution are discarded.
      </p>
      <div class="grid-2" style="align-items:start;">
        <figure>
          <img src="media/831763004403_.pic.jpg" alt="Example calibration photo with a 3×2 ArUco grid" loading="lazy"/>
          <figcaption>Example of calibration image with ArUco tags.</figcaption>
        </figure>
        <div>
          <div class="answer">
            <strong>Implementation summary</strong>
            <ol style="margin:6px 0 0 18px;">
              <li>Detect all markers per frame via OpenCV ArUco; refine 4-corner locations with <code>cornerSubPix</code>.</li>
              <li>Map detected IDs <code>0…5</code> to a row-major 3×2 grid and build world points on the Z=0 plane.
                  The grid origin is the center of the (0,0) tag; each tag’s TL–TR–BR–BL corners are offset by ±L/2.</li>
              <li>Accumulate <code>(objectPoints, imagePoints)</code> across views and call <code>cv2.calibrateCamera</code> to solve for
                  intrinsics <code>K</code> and distortion <code>dist</code>.</li>
              <li>Compute per-image reprojection error; skip frames with missing tags or outlier errors; save
                  <code>part0_calibration.npz/json</code> with <code>K</code>, <code>dist</code>, and diagnostics.</li>
            </ol>
          </div>
          <div class="note">
            Capture rules: fixed zoom, consistent exposure, no motion blur, include the full tag borders, and vary angles to avoid near-degenerate views.
          </div>
        </div>
      </div>
    </section>

    <!-- Part 0.2 — Capturing a 3D Object Scan -->
    <section id="part02">
      <h2>Part 0.2 — Capturing a 3D Object Scan</h2>
      <p>
        I captured a tabletop object with a <strong>single printed ArUco tag</strong> (DICT_4X4_50, 60&nbsp;mm square)
        placed next to the object. I used the <strong>same phone, resolution, and zoom</strong> as in calibration and took
        roughly <strong>30–50 images</strong> from diverse horizontal and vertical angles to achieve good coverage while
        keeping exposure consistent.
      </p>
    
      <div class="grid-2" style="align-items:start;">
        <figure>
          <img src="media/1751763004714_.pic.jpg" alt="Example of my object with a single ArUco tag" loading="lazy" />
          <figcaption>Example of my object with a single ArUco tag.</figcaption>
        </figure>
    
        <div>
          <div class="answer">
            <strong>Capture checklist</strong>
            <ul style="margin:6px 0 0 18px;">
              <li>Keep <em>zoom/focal length fixed</em>; lock exposure if possible to avoid brightness jumps.</li>
              <li>Ensure the tag has a <em>white border</em> and lies flat on the tabletop next to the object.</li>
              <li>Maintain a roughly <em>uniform distance (≈10–20&nbsp;cm)</em> so the object fills about 50% of the frame.</li>
              <li>Vary viewpoints <em>both horizontally and vertically</em>; avoid motion blur.</li>
              <li>Keep the entire tag (with border) visible in every frame for robust detection.</li>
            </ul>
          </div>
          <div class="note">
            Frames are saved under <code>data/Object_Scan2</code> and will be used for pose estimation in Part&nbsp;0.3.
          </div>
        </div>
      </div>
    </section>
    <!-- Part 0.3 — Estimating Camera Pose -->
    <section id="part03">
      <h2>Part 0.3 — Estimating Camera Pose</h2>
      <p>
        With the intrinsics and distortion from Part&nbsp;0.1, I estimated a camera pose
        for every object-frame. This is a PnP setup: match the 2D image corners of the
        single ArUco tag to their known 3D coordinates on the tabletop (meters), solve
        for rotation and translation, and convert to a camera-to-world (<code>c2w</code>)
        matrix for visualization and later NeRF training.
      </p>
    
      <div class="answer">
        <strong>Implementation overview (high level)</strong>
        <ul style="margin:6px 0 0 18px;">
          <li><b>Detect & refine:</b> Detect the single tag in each frame and refine its 4 corners to sub-pixel accuracy.</li>
          <li><b>Pose per frame:</b> Solve a planar PnP (square tag) to obtain world-to-camera extrinsics, then invert to <code>c2w</code>.</li>
          <li><b>Quality gate:</b> Skip frames without the tag and compute per-image reprojection error to catch outliers.</li>
          <li><b>Consistency:</b> Use the same intrinsics/zoom as calibration; keep tag size in meters (here 0.060&nbsp;m) so poses have a consistent scale.</li>
          <li><b>Outputs:</b> Save <code>c2w</code> for each image along with diagnostics to <code>part0_poses.npz/json</code>.</li>
        </ul>
      </div>
    
      <div class="note">
        OpenCV returns world-to-camera by default; for NeRF I use <code>c2w = (w2c)^{-1}</code>. I visualize these poses as
        camera frustums with the source images textured on the image planes.
      </div>
    
      <h3 style="margin-top:14px;">Camera Frustum Visualization</h3>
      <p>
        I rendered the estimated poses in 3D using Viser. Each frustum shows a camera’s position, orientation,
        and its captured image. Below are two views of the same pose cloud.
      </p>
    
      <div class="grid-2" style="align-items:start;">
        <figure>
          <img src="media/cloud_of_cameras1.png" alt="Viser visualization (view 1)" loading="lazy" />
          <figcaption>Viser visualization (view&nbsp;1).</figcaption>
        </figure>
        <figure>
          <img src="media/cloud_of_cameras3.png" alt="Viser visualization (view 2)" loading="lazy" />
          <figcaption>Viser visualization (view&nbsp;2).</figcaption>
        </figure>
      </div>
    </section>
    <!-- Part 0.4 — Undistorting Images and Creating a Dataset -->
    <section id="part04">
      <h2>Part 0.4 — Undistorting Images and Creating a Dataset</h2>
      <p>
        Using the intrinsics and distortion from Part&nbsp;0.1 and the per-frame poses from Part&nbsp;0.3,
        I prepared a clean dataset for NeRF training. The goal is to make all frames consistent with a pinhole
        camera model (no distortion), unify their dimensions, and package images and camera poses together.
      </p>
    
      <div class="answer">
        <strong>What I did (high level)</strong>
        <ul style="margin:6px 0 0 18px;">
          <li><b>Undistortion:</b> For each frame, remove lens distortion so the images follow the pinhole model.</li>
          <li><b>Optimal intrinsics &amp; crop:</b> Compute an optimal new camera matrix and a valid ROI to avoid black borders;
              then crop all frames to that ROI. Adjust the principal point to account for the crop.</li>
          <li><b>Consistency:</b> Apply the <em>same</em> ROI and new intrinsics to all frames so every image has identical
              resolution. Convert to RGB <code>uint8</code> in <code>[0,255]</code>.</li>
          <li><b>Pair with poses:</b> Keep each image’s <code>c2w</code> pose from Part&nbsp;0.3 (cropping does not change extrinsics).</li>
          <li><b>Split &amp; save:</b> Split frames into train/val/test (≈80/10/10) and save a compact <code>.npz</code>
              that NeRF loaders can consume directly.</li>
        </ul>
      </div>
    </section>
    </section>

    <!-- ======================= Part 1 ======================= -->
    <section id="part1">
      <h2>Part 1: Fit a Neural Field to a 2D Image</h2>
    
      <!-- 1) Model & Training Setup -->
      <h3>Model & Training Setup</h3>
      <p>
        We fit a 2D neural field with sinusoidal positional encoding (PE) and an MLP.
        The output is constrained by a <code>Sigmoid</code> to [0,1].
      </p>
      <ul>
        <li><b>Positional Encoding (max frequency L)</b>: keep (x,y) and add <code>sin(2^lπ·)</code>, <code>cos(2^lπ·)</code> for <code>l = 0..L-1</code>. Input dim = <code>2 + 4L</code>. (Example: L=10 → 42)</li>
        <li><b>MLP</b>: <code>[Linear(width)+ReLU] × K</code> → <code>Linear(3)</code> → <code>Sigmoid</code>. (Shown below)</li>
        <li><b>Loss</b>: <code>MSELoss</code>；<b>Optimizer</b>: <code>Adam(lr = 1e-2)</code>；<b>Batch</b>: 10k random pixels/iter；<b>Iters</b>: 1000–3000 (ours: 2000)。</li>
      </ul>
    
      <!-- concise arch table -->
      <div class="table-like">
        <table>
          <thead>
            <tr><th>Component</th><th>Setting</th></tr>
          </thead>
          <tbody>
            <tr><td>Max PE frequency (L)</td><td><span class="hl">L = 10</span> (and we also compare a low L, e.g., 3)</td></tr>
            <tr><td># Hidden layers (K)</td><td><span class="hl">3</span> or <span class="hl">4</span> (Linear+ReLU blocks)</td></tr>
            <tr><td>Width</td><td><span class="hl">256</span> (also compare a lower width, e.g., 64)</td></tr>
            <tr><td>Activation</td><td>ReLU (hidden), Sigmoid (output)</td></tr>
            <tr><td>Loss / Optimizer</td><td>MSE / Adam (lr = 1e-2)</td></tr>
            <tr><td>Batch size</td><td>10,000 random pixels per iteration</td></tr>
            <tr><td>Iters</td><td>2000</td></tr>
          </tbody>
        </table>
      </div>
    
      <!-- 2) Training progression on provided test image -->
      <h3>Training Progression — Provided Test Image</h3>
      <p>Snapshots at different iterations (from left to right): initialization → early → mid → final.</p>
      <div class="grid-4">
        <figure>
          <img src="outputs/part1_notebook/iter_0000.png" alt="Init prediction (test image)">
          <figcaption>Init (before training)</figcaption>
        </figure>
        <figure>
          <img src="outputs/part1_notebook/iter_0200.png" alt="Iter 200 (test image)">
          <figcaption>Iter 200</figcaption>
        </figure>
        <figure>
          <img src="outputs/part1_notebook/iter_0600.png" alt="Iter 600 (test image)">
          <figcaption>Iter 600</figcaption>
        </figure>
        <figure>
          <img src="outputs/part1_notebook/final.png" alt="Final reconstruction (test image)">
          <figcaption>Final</figcaption>
        </figure>
      </div>
    
      <!-- 3) Training progression on my own image -->
      <h3>Training Progression — My Own Image</h3>
      <p>Replace the following paths with your second run’s outputs.</p>
      <div class="grid-4">
        <figure>
          <img src="outputs/part1_notebook_myimg/iter_0000.png" alt="Init prediction (own image)">
          <figcaption>Init</figcaption>
        </figure>
        <figure>
          <img src="outputs/part1_notebook_myimg/iter_0200.png" alt="Iter 200 (own image)">
          <figcaption>Iter 200</figcaption>
        </figure>
        <figure>
          <img src="outputs/part1_notebook_myimg/iter_0600.png" alt="Iter 600 (own image)">
          <figcaption>Iter 600</figcaption>
        </figure>
        <figure>
          <img src="outputs/part1_notebook_myimg/final.png" alt="Final reconstruction (own image)">
          <figcaption>Final</figcaption>
        </figure>
      </div>
    
      <!-- 4) 2x2 grid: two L x two widths -->
      <h3>Effect of Max PE Frequency (L) and Width</h3>
      <p>
        Final reconstructions for two choices of <b>L</b> and two choices of <b>width</b> (2×2).  
        Here we tried a very low L and a small width to illustrate the low-frequency/low-capacity limit.
      </p>
      <figure class="center">
        <img src="outputs/part1_notebook/sweep/grid_LxW.png" alt="2x2 grid of final results for two L and two widths">
        <figcaption>2×2 grid: (L ∈ {3, 10}) × (width ∈ {64, 256}). Replace with your actual sweep outputs.</figcaption>
      </figure>
    
      <!-- 5) PSNR curve -->
      <h3>Training PSNR</h3>
      <p>PSNR computed per-iteration on a fixed validation subset; we also evaluate full-image PSNR periodically.</p>
      <figure class="center">
        <img src="outputs/part1_notebook/psnr_curve.png" alt="PSNR curve over training iterations">
        <figcaption>PSNR vs Iteration for one image of choice.</figcaption>
      </figure>
    
      <!-- 6) Notes -->
      <h3>Notes & Observations</h3>
      <ul>
        <li>Low <b>L</b> enforces a strong low-pass prior—smooth colors, blurry edges; higher <b>L</b> restores high-frequency details but may introduce grid-like artifacts if too high.</li>
        <li>Smaller <b>width</b> (capacity) underfits fine structures; larger width converges to higher PSNR and sharper details.</li>
        <li>Initialization yields a nearly constant gray prediction (Sigmoid around 0.5), thus initial PSNR ≈ 9–11 dB.</li>
      </ul>
    </section>

    
    


    <!-- Future sections placeholders (do not render yet) -->
    <!--
    <section id="part1"><h2>Part 1 — NeRF Training</h2></section>
    <section id="part2"><h2>Part 2 — Improvements & Novel Views</h2></section>
    -->
  </main>

  <footer>
    © <script>document.write(new Date().getFullYear());</script> Project 4. All images and results by the author.
  </footer>
</body>
</html>

