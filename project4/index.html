<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Project 4 — Neural Radiance Field (NeRF)</title>
  <style>
    :root { --maxw: 980px; }
    body {
      font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif;
      margin: 0; line-height: 1.6; color: #222; background: #fafafa;
    }
    header, main, footer { max-width: var(--maxw); margin: 0 auto; padding: 20px; }
    header h1 { margin: 0 0 6px; }
    header a { color: #0b5fff; text-decoration: none; }
    .back { display: inline-block; margin-top: 6px; }
    section {
      background: #fff; border: 1px solid #eee; border-radius: 14px;
      padding: 18px; margin: 18px 0 24px; box-shadow: 0 1px 3px rgba(0,0,0,.05);
    }
    h2 { margin: 0 0 12px; font-size: 1.25rem; }
    p { margin: 8px 0; }
    .grid-2 { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 12px; }
    @media (max-width: 720px) { .grid-2 { grid-template-columns: 1fr; } }

    figure { margin: 0; border: 1px solid #eee; border-radius: 12px; overflow: hidden; background: #fff; }
    figure img, figure video { display: block; width: 100%; height: auto; }
    figcaption { font-size: .9rem; padding: 10px 12px; background: #f6f7fb; border-top: 1px solid #eee; }

    .answer { background: #f4fbf6; border: 1px solid #d8f1df; border-radius: 10px; padding: 10px 12px; margin-top: 10px; }
    .note { font-size: .92rem; color: #555; background: #fff7e6; border: 1px solid #ffe2a8; border-radius: 10px; padding: 10px 12px; margin-top: 10px; }

    /* Tables */
    table { width: 100%; border-collapse: collapse; }
    thead th { text-align: left; font-weight: 600; padding: 10px 10px; background: #f6f7fb; border-bottom: 1px solid #eee; }
    tbody td { padding: 10px; border-top: 1px solid #eee; vertical-align: middle; }
    td img { width: 240px; height: auto; display: block; border-radius: 10px; border: 1px solid #eee; }
    td.number, td.time { white-space: nowrap; font-variant-numeric: tabular-nums; }
    @media (max-width: 720px) { td img { width: 100%; } }

    footer { font-size: .9rem; color: #666; }
  </style>
</head>
<body>
  <header>
    <h1>Project 4 — Neural Radiance Field (NeRF)</h1>
    <a class="back" href="../index.html">← Back to main portfolio</a>
  </header>

  <main>

    <!-- Part 0 title only -->
    <section id="part0">
      <h2>Part 0 — Calibrating Your Camera, Capturing a 3D Scan, Estimating Poses, and Building the Dataset</h2>
      <!-- Content to be filled in next iteration: 0.1 Calibration, 0.2 Capture, 0.3 Pose (Viser frustums), 0.4 Undistort & NPZ packaging -->
      <!-- Part 0.1 -->
      <section id="part01">
      <h2>Part 0.1 — Calibrating Your Camera</h2>
      <p>
        I calibrate the phone camera using a printed 3×2 grid of ArUco tags (DICT_4X4_50), sized 
        <strong>60&nbsp;mm</strong> square each, with center-to-center spacing of <strong>90.00&nbsp;mm (H)</strong> and <strong>75.67&nbsp;mm (V)</strong>.
        I captured <strong>30–50 images</strong> at a fixed zoom and resolution, while varying viewing angle and distance to
        improve numerical conditioning. Blurry frames or images with mismatched resolution are discarded.
      </p>
      <div class="grid-2" style="align-items:start;">
        <figure>
          <img src="media/831763004403_.pic.jpg" alt="Example calibration photo with a 3×2 ArUco grid" loading="lazy"/>
          <figcaption>Example of calibration image with ArUco tags.</figcaption>
        </figure>
        <div>
          <div class="answer">
            <strong>Implementation summary</strong>
            <ol style="margin:6px 0 0 18px;">
              <li>Detect all markers per frame via OpenCV ArUco; refine 4-corner locations with <code>cornerSubPix</code>.</li>
              <li>Map detected IDs <code>0…5</code> to a row-major 3×2 grid and build world points on the Z=0 plane.
                  The grid origin is the center of the (0,0) tag; each tag’s TL–TR–BR–BL corners are offset by ±L/2.</li>
              <li>Accumulate <code>(objectPoints, imagePoints)</code> across views and call <code>cv2.calibrateCamera</code> to solve for
                  intrinsics <code>K</code> and distortion <code>dist</code>.</li>
              <li>Compute per-image reprojection error; skip frames with missing tags or outlier errors; save
                  <code>part0_calibration.npz/json</code> with <code>K</code>, <code>dist</code>, and diagnostics.</li>
            </ol>
          </div>
          <div class="note">
            Capture rules: fixed zoom, consistent exposure, no motion blur, include the full tag borders, and vary angles to avoid near-degenerate views.
          </div>
        </div>
      </div>
    </section>

    <!-- Part 0.2 — Capturing a 3D Object Scan -->
    <section id="part02">
      <h2>Part 0.2 — Capturing a 3D Object Scan</h2>
      <p>
        I captured a tabletop object with a <strong>single printed ArUco tag</strong> (DICT_4X4_50, 60&nbsp;mm square)
        placed next to the object. I used the <strong>same phone, resolution, and zoom</strong> as in calibration and took
        roughly <strong>30–50 images</strong> from diverse horizontal and vertical angles to achieve good coverage while
        keeping exposure consistent.
      </p>
    
      <div class="grid-2" style="align-items:start;">
        <figure>
          <img src="media/1751763004714_.pic.jpg" alt="Example of my object with a single ArUco tag" loading="lazy" />
          <figcaption>Example of my object with a single ArUco tag.</figcaption>
        </figure>
    
        <div>
          <div class="answer">
            <strong>Capture checklist</strong>
            <ul style="margin:6px 0 0 18px;">
              <li>Keep <em>zoom/focal length fixed</em>; lock exposure if possible to avoid brightness jumps.</li>
              <li>Ensure the tag has a <em>white border</em> and lies flat on the tabletop next to the object.</li>
              <li>Maintain a roughly <em>uniform distance (≈10–20&nbsp;cm)</em> so the object fills about 50% of the frame.</li>
              <li>Vary viewpoints <em>both horizontally and vertically</em>; avoid motion blur.</li>
              <li>Keep the entire tag (with border) visible in every frame for robust detection.</li>
            </ul>
          </div>
          <div class="note">
            Frames are saved under <code>data/Object_Scan2</code> and will be used for pose estimation in Part&nbsp;0.3.
          </div>
        </div>
      </div>
    </section>
    <!-- Part 0.3 — Estimating Camera Pose -->
    <section id="part03">
      <h2>Part 0.3 — Estimating Camera Pose</h2>
      <p>
        With the intrinsics and distortion from Part&nbsp;0.1, I estimated a camera pose
        for every object-frame. This is a PnP setup: match the 2D image corners of the
        single ArUco tag to their known 3D coordinates on the tabletop (meters), solve
        for rotation and translation, and convert to a camera-to-world (<code>c2w</code>)
        matrix for visualization and later NeRF training.
      </p>
    
      <div class="answer">
        <strong>Implementation overview (high level)</strong>
        <ul style="margin:6px 0 0 18px;">
          <li><b>Detect & refine:</b> Detect the single tag in each frame and refine its 4 corners to sub-pixel accuracy.</li>
          <li><b>Pose per frame:</b> Solve a planar PnP (square tag) to obtain world-to-camera extrinsics, then invert to <code>c2w</code>.</li>
          <li><b>Quality gate:</b> Skip frames without the tag and compute per-image reprojection error to catch outliers.</li>
          <li><b>Consistency:</b> Use the same intrinsics/zoom as calibration; keep tag size in meters (here 0.060&nbsp;m) so poses have a consistent scale.</li>
          <li><b>Outputs:</b> Save <code>c2w</code> for each image along with diagnostics to <code>part0_poses.npz/json</code>.</li>
        </ul>
      </div>
    
      <div class="note">
        OpenCV returns world-to-camera by default; for NeRF I use <code>c2w = (w2c)^{-1}</code>. I visualize these poses as
        camera frustums with the source images textured on the image planes.
      </div>
    
      <h3 style="margin-top:14px;">Camera Frustum Visualization</h3>
      <p>
        I rendered the estimated poses in 3D using Viser. Each frustum shows a camera’s position, orientation,
        and its captured image. Below are two views of the same pose cloud.
      </p>
    
      <div class="grid-2" style="align-items:start;">
        <figure>
          <img src="media/cloud_of_cameras1.png" alt="Viser visualization (view 1)" loading="lazy" />
          <figcaption>Viser visualization (view&nbsp;1).</figcaption>
        </figure>
        <figure>
          <img src="media/cloud_of_cameras3.png" alt="Viser visualization (view 2)" loading="lazy" />
          <figcaption>Viser visualization (view&nbsp;2).</figcaption>
        </figure>
      </div>
    </section>
    <!-- Part 0.4 — Undistorting Images and Creating a Dataset -->
    <section id="part04">
      <h2>Part 0.4 — Undistorting Images and Creating a Dataset</h2>
      <p>
        Using the intrinsics and distortion from Part&nbsp;0.1 and the per-frame poses from Part&nbsp;0.3,
        I prepared a clean dataset for NeRF training. The goal is to make all frames consistent with a pinhole
        camera model (no distortion), unify their dimensions, and package images and camera poses together.
      </p>
    
      <div class="answer">
        <strong>What I did (high level)</strong>
        <ul style="margin:6px 0 0 18px;">
          <li><b>Undistortion:</b> For each frame, remove lens distortion so the images follow the pinhole model.</li>
          <li><b>Optimal intrinsics &amp; crop:</b> Compute an optimal new camera matrix and a valid ROI to avoid black borders;
              then crop all frames to that ROI. Adjust the principal point to account for the crop.</li>
          <li><b>Consistency:</b> Apply the <em>same</em> ROI and new intrinsics to all frames so every image has identical
              resolution. Convert to RGB <code>uint8</code> in <code>[0,255]</code>.</li>
          <li><b>Pair with poses:</b> Keep each image’s <code>c2w</code> pose from Part&nbsp;0.3 (cropping does not change extrinsics).</li>
          <li><b>Split &amp; save:</b> Split frames into train/val/test (≈80/10/10) and save a compact <code>.npz</code>
              that NeRF loaders can consume directly.</li>
        </ul>
      </div>
    </section>

    </section>
    
    


    <!-- Future sections placeholders (do not render yet) -->
    <!--
    <section id="part1"><h2>Part 1 — NeRF Training</h2></section>
    <section id="part2"><h2>Part 2 — Improvements & Novel Views</h2></section>
    -->
  </main>

  <footer>
    © <script>document.write(new Date().getFullYear());</script> Project 4. All images and results by the author.
  </footer>
</body>
</html>

