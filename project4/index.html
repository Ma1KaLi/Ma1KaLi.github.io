<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Project 4 — Neural Radiance Field (NeRF)</title>
  <style>
    :root { --maxw: 980px; }
    body {
      font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif;
      margin: 0; line-height: 1.6; color: #222; background: #fafafa;
    }
    header, main, footer { max-width: var(--maxw); margin: 0 auto; padding: 20px; }
    header h1 { margin: 0 0 6px; }
    header a { color: #0b5fff; text-decoration: none; }
    .back { display: inline-block; margin-top: 6px; }
    /* 让这个 GIF 在 cell 中缩小并保持比例、居中 */
    .small-gif {
        max-width: 300px;   /* 控制大小：可以改成 250 / 350 等 */
        width: 100%;        /* 在小屏上可以自适应缩放，但不会超过 max-width */
        height: auto;       /* 保持宽高比 */
        display: block;     /* 为了下面的 margin 居中 */
        margin: 0 auto;     /* 水平居中 */
    }
    section {
      background: #fff; border: 1px solid #eee; border-radius: 14px;
      padding: 18px; margin: 18px 0 24px; box-shadow: 0 1px 3px rgba(0,0,0,.05);
    }
    h2 { margin: 0 0 12px; font-size: 1.25rem; }
    p { margin: 8px 0; }
    .grid-2 { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 12px; }
    @media (max-width: 720px) { .grid-2 { grid-template-columns: 1fr; } }

    figure { margin: 0; border: 1px solid #eee; border-radius: 12px; overflow: hidden; background: #fff; }
    figure img, figure video { display: block; width: 100%; height: auto; }
    figcaption { font-size: .9rem; padding: 10px 12px; background: #f6f7fb; border-top: 1px solid #eee; }

    .answer { background: #f4fbf6; border: 1px solid #d8f1df; border-radius: 10px; padding: 10px 12px; margin-top: 10px; }
    .note { font-size: .92rem; color: #555; background: #fff7e6; border: 1px solid #ffe2a8; border-radius: 10px; padding: 10px 12px; margin-top: 10px; }

    /* Tables */
    table { width: 100%; border-collapse: collapse; }
    thead th { text-align: left; font-weight: 600; padding: 10px 10px; background: #f6f7fb; border-bottom: 1px solid #eee; }
    tbody td { padding: 10px; border-top: 1px solid #eee; vertical-align: middle; }
    td img { width: 240px; height: auto; display: block; border-radius: 10px; border: 1px solid #eee; }
    td.number, td.time { white-space: nowrap; font-variant-numeric: tabular-nums; }
    @media (max-width: 720px) { td img { width: 100%; } }

    footer { font-size: .9rem; color: #666; }
  </style>
</head>
<body>
  <header>
    <h1>Project 4 — Neural Radiance Field (NeRF)</h1>
    <a class="back" href="../index.html">← Back to main portfolio</a>
  </header>

  <main>

    <!-- Part 0 title only -->
    <section id="part0">
      <h2>Part 0 — Calibrating Your Camera, Capturing a 3D Scan, Estimating Poses, and Building the Dataset</h2>
      <!-- Content to be filled in next iteration: 0.1 Calibration, 0.2 Capture, 0.3 Pose (Viser frustums), 0.4 Undistort & NPZ packaging -->
      <!-- Part 0.1 -->
      <section id="part01">
      <h2>Part 0.1 — Calibrating Your Camera</h2>
      <p>
        I calibrate the phone camera using a printed 3×2 grid of ArUco tags (DICT_4X4_50), sized 
        <strong>60&nbsp;mm</strong> square each, with center-to-center spacing of <strong>90.00&nbsp;mm (H)</strong> and <strong>75.67&nbsp;mm (V)</strong>.
        I captured <strong>30–50 images</strong> at a fixed zoom and resolution, while varying viewing angle and distance to
        improve numerical conditioning. Blurry frames or images with mismatched resolution are discarded.
      </p>
      <div class="grid-2" style="align-items:start;">
        <figure>
          <img src="media/831763004403_.pic.jpg" alt="Example calibration photo with a 3×2 ArUco grid" loading="lazy"/>
          <figcaption>Example of calibration image with ArUco tags.</figcaption>
        </figure>
        <div>
          <div class="answer">
            <strong>Implementation summary</strong>
            <ol style="margin:6px 0 0 18px;">
              <li>Detect all markers per frame via OpenCV ArUco; refine 4-corner locations with <code>cornerSubPix</code>.</li>
              <li>Map detected IDs <code>0…5</code> to a row-major 3×2 grid and build world points on the Z=0 plane.
                  The grid origin is the center of the (0,0) tag; each tag’s TL–TR–BR–BL corners are offset by ±L/2.</li>
              <li>Accumulate <code>(objectPoints, imagePoints)</code> across views and call <code>cv2.calibrateCamera</code> to solve for
                  intrinsics <code>K</code> and distortion <code>dist</code>.</li>
              <li>Compute per-image reprojection error; skip frames with missing tags or outlier errors; save
                  <code>part0_calibration.npz/json</code> with <code>K</code>, <code>dist</code>, and diagnostics.</li>
            </ol>
          </div>
          <div class="note">
            Capture rules: fixed zoom, consistent exposure, no motion blur, include the full tag borders, and vary angles to avoid near-degenerate views.
          </div>
        </div>
      </div>
    </section>

    <!-- Part 0.2 — Capturing a 3D Object Scan -->
    <section id="part02">
      <h2>Part 0.2 — Capturing a 3D Object Scan</h2>
      <p>
        I captured a tabletop object with a <strong>single printed ArUco tag</strong> (DICT_4X4_50, 60&nbsp;mm square)
        placed next to the object. I used the <strong>same phone, resolution, and zoom</strong> as in calibration and took
        roughly <strong>30–50 images</strong> from diverse horizontal and vertical angles to achieve good coverage while
        keeping exposure consistent.
      </p>
    
      <div class="grid-2" style="align-items:start;">
        <figure>
          <img src="media/1751763004714_.pic.jpg" alt="Example of my object with a single ArUco tag" loading="lazy" />
          <figcaption>Example of my object with a single ArUco tag.</figcaption>
        </figure>
    
        <div>
          <div class="answer">
            <strong>Capture checklist</strong>
            <ul style="margin:6px 0 0 18px;">
              <li>Keep <em>zoom/focal length fixed</em>; lock exposure if possible to avoid brightness jumps.</li>
              <li>Ensure the tag has a <em>white border</em> and lies flat on the tabletop next to the object.</li>
              <li>Maintain a roughly <em>uniform distance (≈10–20&nbsp;cm)</em> so the object fills about 50% of the frame.</li>
              <li>Vary viewpoints <em>both horizontally and vertically</em>; avoid motion blur.</li>
              <li>Keep the entire tag (with border) visible in every frame for robust detection.</li>
            </ul>
          </div>
          <div class="note">
            Frames are saved under <code>data/Object_Scan2</code> and will be used for pose estimation in Part&nbsp;0.3.
          </div>
        </div>
      </div>
    </section>
    <!-- Part 0.3 — Estimating Camera Pose -->
    <section id="part03">
      <h2>Part 0.3 — Estimating Camera Pose</h2>
      <p>
        With the intrinsics and distortion from Part&nbsp;0.1, I estimated a camera pose
        for every object-frame. This is a PnP setup: match the 2D image corners of the
        single ArUco tag to their known 3D coordinates on the tabletop (meters), solve
        for rotation and translation, and convert to a camera-to-world (<code>c2w</code>)
        matrix for visualization and later NeRF training.
      </p>
    
      <div class="answer">
        <strong>Implementation overview (high level)</strong>
        <ul style="margin:6px 0 0 18px;">
          <li><b>Detect & refine:</b> Detect the single tag in each frame and refine its 4 corners to sub-pixel accuracy.</li>
          <li><b>Pose per frame:</b> Solve a planar PnP (square tag) to obtain world-to-camera extrinsics, then invert to <code>c2w</code>.</li>
          <li><b>Quality gate:</b> Skip frames without the tag and compute per-image reprojection error to catch outliers.</li>
          <li><b>Consistency:</b> Use the same intrinsics/zoom as calibration; keep tag size in meters (here 0.060&nbsp;m) so poses have a consistent scale.</li>
          <li><b>Outputs:</b> Save <code>c2w</code> for each image along with diagnostics to <code>part0_poses.npz/json</code>.</li>
        </ul>
      </div>
    
      <div class="note">
        OpenCV returns world-to-camera by default; for NeRF I use <code>c2w = (w2c)^{-1}</code>. I visualize these poses as
        camera frustums with the source images textured on the image planes.
      </div>
    
      <h3 style="margin-top:14px;">Camera Frustum Visualization</h3>
      <p>
        I rendered the estimated poses in 3D using Viser. Each frustum shows a camera’s position, orientation,
        and its captured image. Below are two views of the same pose cloud.
      </p>
    
      <div class="grid-2" style="align-items:start;">
        <figure>
          <img src="media/cloud_of_cameras1.png" alt="Viser visualization (view 1)" loading="lazy" />
          <figcaption>Viser visualization (view&nbsp;1).</figcaption>
        </figure>
        <figure>
          <img src="media/cloud_of_cameras3.png" alt="Viser visualization (view 2)" loading="lazy" />
          <figcaption>Viser visualization (view&nbsp;2).</figcaption>
        </figure>
      </div>
    </section>
    <!-- Part 0.4 — Undistorting Images and Creating a Dataset -->
    <section id="part04">
      <h2>Part 0.4 — Undistorting Images and Creating a Dataset</h2>
      <p>
        Using the intrinsics and distortion from Part&nbsp;0.1 and the per-frame poses from Part&nbsp;0.3,
        I prepared a clean dataset for NeRF training. The goal is to make all frames consistent with a pinhole
        camera model (no distortion), unify their dimensions, and package images and camera poses together.
      </p>
    
      <div class="answer">
        <strong>What I did (high level)</strong>
        <ul style="margin:6px 0 0 18px;">
          <li><b>Undistortion:</b> For each frame, remove lens distortion so the images follow the pinhole model.</li>
          <li><b>Optimal intrinsics &amp; crop:</b> Compute an optimal new camera matrix and a valid ROI to avoid black borders;
              then crop all frames to that ROI. Adjust the principal point to account for the crop.</li>
          <li><b>Consistency:</b> Apply the <em>same</em> ROI and new intrinsics to all frames so every image has identical
              resolution. Convert to RGB <code>uint8</code> in <code>[0,255]</code>.</li>
          <li><b>Pair with poses:</b> Keep each image’s <code>c2w</code> pose from Part&nbsp;0.3 (cropping does not change extrinsics).</li>
          <li><b>Split &amp; save:</b> Split frames into train/val/test (≈80/10/10) and save a compact <code>.npz</code>
              that NeRF loaders can consume directly.</li>
        </ul>
      </div>
    </section>
    </section>

    <!-- ======================= Part 1 ======================= -->
    <section id="part1">
      <h2>Part 1: Fit a Neural Field to a 2D Image</h2>
    
      <!-- Network Architecture intro -->
      <h3>Network Architecture</h3>
      <p>
        We learn a 2D neural field mapping normalized pixel coordinates <code>(x,y)∈[0,1]^2</code> to RGB in <code>[0,1]</code>.
        Coordinates are expanded by sinusoidal positional encoding (PE): for levels <code>l=0…L−1</code> we append
        <code>sin(2^lπ·)</code> and <code>cos(2^lπ·)</code> for each axis while keeping the raw <code>(x,y)</code>, yielding input
        dimension <code>2 + 4L</code>. The encoder output is fed to an MLP:
      </p>
      <ul>
        <li><b>MLP:</b> <code>[Linear(width) + ReLU] × K</code> → <code>Linear(3)</code> → <code>Sigmoid</code> (bounds to [0,1]).</li>
        <li><b>Loss / Optimizer:</b> MSELoss / Adam with <code>lr = 1e-2</code>.</li>
        <li><b>Training:</b> each iteration randomly samples 10k pixels (coordinates + RGB) from the image.</li>
      </ul>
    
      <div class="table-like">
        <table>
          <thead><tr><th>Hyperparameter</th><th>Value (example)</th></tr></thead>
          <tbody>
            <tr><td>Max PE level <em>L</em></td><td>10 (also compare a low L such as 3)</td></tr>
            <tr><td>Hidden blocks <em>K</em></td><td>4 (Linear+ReLU)</td></tr>
            <tr><td>Width</td><td>256 (also compare 64)</td></tr>
            <tr><td>Learning rate</td><td>1e-2 (Adam)</td></tr>
            <tr><td>Batch size</td><td>10,000 pixels / iter</td></tr>
            <tr><td>Iterations</td><td>3000</td></tr>
          </tbody>
        </table>
      </div>
    
      <!-- 2×2: separate images for L × width (moved here, just after architecture) -->
      <h3>Effect of Max PE Frequency (L) and Width</h3>
      <p>
        To understand how frequency content and model capacity trade off in reconstruction, we vary the maximum PE level
        <b>L</b> and the hidden-layer <b>width</b>. The 2×2 comparison below uses <b>L ∈ {3, 10}</b> and
        <b>width ∈ {64, 256}</b> on the staff image.
      </p>
      <div class="grid-2">
        <figure>
          <img src="media/final_L3_W64.png" alt="L=3, width=64">
          <figcaption>L = 3, width = 64</figcaption>
        </figure>
        <figure>
          <img src="media/final_L3_W256.png" alt="L=3, width=256">
          <figcaption>L = 3, width = 256</figcaption>
        </figure>
        <figure>
          <img src="media/final_L10_W64.png" alt="L=10, width=64">
          <figcaption>L = 10, width = 64</figcaption>
        </figure>
        <figure>
          <img src="media/final_L10_W256.png" alt="L=10, width=256">
          <figcaption>L = 10, width = 256</figcaption>
        </figure>
      </div>
    
      <!-- Training progression: Provided Test Image (2x2: 200/400/1000/3000) -->
      <h3>Training Progression — Provided Test Image</h3>
      <div class="grid-2">
        <figure>
          <img src="media/iter_0200.png" alt="Test image iteration 200">
          <figcaption>Iteration 200</figcaption>
        </figure>
        <figure>
          <img src="media/iter_0400.png" alt="Test image iteration 400">
          <figcaption>Iteration 400</figcaption>
        </figure>
        <figure>
          <img src="media/iter_1000.png" alt="Test image iteration 1000">
          <figcaption>Iteration 1000</figcaption>
        </figure>
        <figure>
          <img src="media/iter_3000.png" alt="Test image iteration 3000">
          <figcaption>Iteration 3000</figcaption>
        </figure>
      </div>
    
      <!-- Training progression: My Own Image (2x2: 200/400/1000/3000) -->
      <h3>Training Progression — My Own Image</h3>
      <div class="grid-2">
        <figure>
          <img src="media/iter_cl_0200.png" alt="Own image iteration 200">
          <figcaption>Iteration 200</figcaption>
        </figure>
        <figure>
          <img src="media/iter_cl_0400.png" alt="Own image iteration 400">
          <figcaption>Iteration 400</figcaption>
        </figure>
        <figure>
          <img src="media/iter_cl_1000.png" alt="Own image iteration 1000">
          <figcaption>Iteration 1000</figcaption>
        </figure>
        <figure>
          <img src="media/iter_cl_3000.png" alt="Own image iteration 3000">
          <figcaption>Iteration 3000</figcaption>
        </figure>
      </div>
    
      <!-- Two PSNR curves in one horizontal row -->
      <h3>Training PSNR</h3>
      <p>
        The plots below track PSNR over the course of training—a fidelity measure comparing the 
        reconstruction to the ground-truth image. Higher PSNR means lower error and therefore 
        better reconstruction quality.
      </p>
      <div class="row">
        <figure>
          <img src="media/psnr_curve.png" alt="PSNR curve for test image">
          <figcaption>PSNR vs Iteration — Test Image</figcaption>
        </figure>
        <figure>
          <img src="media/psnr_curve_cl.png" alt="PSNR curve for my own image">
          <figcaption>PSNR vs Iteration — My Own Image</figcaption>
        </figure>
      </div>
    
      <!-- Final results 2×2 -->
      <h3>Final Results</h3>
      <p>
        Below are the source photographs and the reconstructions after <b>3000 iterations</b> using
        <b>L = 10</b>, <b>width = 256</b>, and the same PE+MLP configuration described above.
      </p>
      <div class="grid-2">
        <figure>
          <img src="media/test.jpg" alt="Test image original">
          <figcaption>Test Image (Original)</figcaption>
        </figure>
        <figure>
          <img src="media/final.png" alt="Test image final reconstruction">
          <figcaption>Test Image (Final)</figcaption>
        </figure>
        <figure>
          <img src="media/my_own.jpg" alt="My own image original">
          <figcaption>My Own Image (Original)</figcaption>
        </figure>
        <figure>
          <img src="media/final_cl.png" alt="My own image final reconstruction">
          <figcaption>My Own Image (Final)</figcaption>
        </figure>
      </div>
    </section>

    <!-- ======================= Part 2: Neural Radiance Field on Lego ======================= -->
    <section id="part2">
        <h2>Part 2: Fit a Neural Radiance Field from Multi-view Images</h2>
        <p>
            In this part, I extend the 2D neural field from Part 1 into a full Neural Radiance Field (NeRF)
            that models a 3D Lego scene from multi-view calibrated images. Given camera intrinsics, camera-to-world
            (c2w) matrices, and posed images, I implement the full NeRF rendering pipeline: ray generation,
            stratified sampling along rays, a 3D neural field that predicts color and density, and differentiable
            volume rendering. The final model can synthesize novel views of the Lego from unseen camera poses.
        </p>
    
        <!-- ======================= Part 2.1 ======================= -->
        <section id="part2-1">
            <h3>Part 2.1: Create Rays from Cameras</h3>
            <p>
                To render a 3D scene from calibrated cameras, I first convert image pixels into 3D rays in world space.
                This involves three coordinate systems: pixel coordinates, camera coordinates, and world coordinates.
                I implement all transformations in a batched fashion so that entire images or large sets of rays can
                be processed efficiently with NumPy.
            </p>
    
            <h4>Camera to World Coordinates</h4>
            <p>
                Each camera pose is provided as a camera-to-world transformation matrix <code>c2w</code>, which consists
                of a rotation <code>R</code> and translation <code>t</code>. I implemented a function
                <code>x_world = transform(c2w, x_cam)</code> that applies
                <code>x_world = R &times; x_cam + t</code> to batched 3D points. To correctly handle batches of points
                and matrices, I use <code>np.einsum</code> instead of explicit Python loops, which makes the
                implementation vectorized and efficient.
            </p>
    
            <h4>Pixel to Camera Coordinates</h4>
            <p>
                Given the intrinsic matrix <code>K</code> and pixel coordinates <code>(u, v)</code>, I implemented
                <code>x_cam = pixel_to_camera(K, uv, s)</code> to map pixels to camera coordinates at a chosen depth
                <code>s</code>. I compute the inverse of <code>K</code> analytically and use simple NumPy operations
                to convert pixel centers <code>(u + 0.5, v + 0.5)</code> into normalized camera-space points. For
                ray directions, I fix <code>s = 1.0</code> so that I get a canonical point along each ray.
            </p>
    
            <h4>Pixel to Ray Conversion</h4>
            <p>
                A ray is defined by an origin <code>r<sub>o</sub></code> and a direction <code>r<sub>d</sub></code>.
                For a pinhole camera, the ray origin is simply the camera center in world space, which I take as the
                translation part of <code>c2w</code>. To get the direction, I:
            </p>
            <ul>
                <li>Convert a pixel to a 3D point in camera coordinates at depth 1.0 with <code>pixel_to_camera</code>.</li>
                <li>Transform that point into world coordinates using <code>transform(c2w, x_cam)</code>.</li>
                <li>Compute the direction as <code>r<sub>d</sub> = normalize(x_world - r<sub>o</sub>)</code>.</li>
            </ul>
            <p>
                The implementation supports both single rays and batched inputs where <code>c2w</code> and
                <code>uv</code> have leading batch dimensions. This batched design is reused later when generating
                rays for entire images and for ray-based training.
            </p>
        </section>
    
        <!-- ======================= Part 2.2 ======================= -->
        <section id="part2-2">
            <h3>Part 2.2: Sampling Rays and Points Along Rays</h3>
            <p>
                With pixel-to-ray conversion in place, the next step is to sample rays from the training images and
                discretize each ray into a set of 3D points. These sampled points will be queried by the NeRF network
                to produce color and density values.
            </p>
    
            <h4>Sampling Rays from Multi-view Images</h4>
            <p>
                I implemented two ray-sampling strategies:
            </p>
            <ul>
                <li>
                    <b>Global sampling:</b> flatten all pixels across all training images and uniformly sample indices
                    to get a global set of rays. This is simple but can oversample certain views when batch size is small.
                </li>
                <li>
                    <b>Grouped sampling (used for training):</b> randomly choose <code>M</code> cameras, then sample
                    roughly <code>N / M</code> pixels from each image. This guarantees that each mini-batch sees rays
                    from multiple views, which makes training more stable. In my experiments I typically used
                    <code>M = 8</code> cameras per batch.
                </li>
            </ul>
            <p>
                In both cases, I use <code>pixel_to_ray</code> to convert the sampled pixels into ray origins and
                directions, and I gather the corresponding RGB colors from the training images as supervision targets.
            </p>
    
            <h4>Sampling Points Along Rays</h4>
            <p>
                For a ray parameterized as <code>r(t) = r<sub>o</sub> + t &middot; r<sub>d</sub></code>, I sample
                <code>n = 64</code> points between a near and far bound. For the Lego scene, I use
                <code>near = 2.0</code> and <code>far = 6.0</code>, which roughly brackets the object in front of
                the cameras. I implemented a stratified sampling function
                <code>sample_along_rays(ray_o, ray_d, near, far, n_samples, perturb)</code> that:
            </p>
            <ul>
                <li>Creates evenly spaced depth values between <code>near</code> and <code>far</code>.</li>
                <li>Optionally perturbs each sample within its bin with uniform noise when <code>perturb=True</code>.</li>
                <li>Computes the 3D points as <code>p = ray_o + t &middot; ray_d</code> in a fully batched manner.</li>
            </ul>
            <p>
                During training, I enable perturbation to approximate the continuous volume integral and improve robustness.
                For validation and rendering, I disable perturbation to get deterministic and smooth images.
            </p> 
        </section>
    
        <!-- ======================= Part 2.3 ======================= -->
        <section id="part2-3">
            <h3>Part 2.3: Dataloader and Ray &amp; Sample Visualization</h3>
            <p>
                After building the ray and point sampling utilities, I combined them into a <code>RaysData</code> dataset
                that serves batches of rays for NeRF training. This dataset is responsible for randomly sampling pixels
                from the multi-view images, converting them to rays, and returning the corresponding RGB colors.
            </p>
    
            <h4>Dataloader Design</h4>
            <p>
                At initialization, the dataset precomputes:
            </p>
            <ul>
                <li>
                    A flattened array of all pixel coordinates <code>uvs</code> for all training images, stored as
                    integer <code>(x, y)</code> indices.
                </li>
                <li>
                    The corresponding target RGB values flattened into a <code>(T &times; H &times; W, 3)</code> tensor.
                </li>
                <li>
                    Precomputed ray origins and directions for every pixel using <code>pixel_to_ray</code>.
                </li>
            </ul>
            <p>
                The method <code>sample_rays(B)</code> randomly samples <code>B</code> indices from this flattened pool
                and returns batched ray origins, directions, and RGB colors, all with shape <code>(B, 3)</code>. I also
                included checks to verify that the flattened indices and colors correctly match the original
                <code>images_train</code> tensor.
            </p>
    
            <h4>Ray and Sample Visualization</h4>
            <p>
                To verify the correctness of ray generation and sampling, I visualized a single training step using
                <code>viser</code>. I randomly sample up to 100 rays from the dataloader, transform them into 3D world
                space, and plot:
            </p>
            <ul>
                <li>The camera frustums for all training/validation/test cameras.</li>
                <li>The 3D line segments representing each ray.</li>
                <li>The 3D sample points along each ray produced by <code>sample_along_rays</code>.</li>
            </ul>
            <p>
                The plot below shows that all sampled rays stay within the camera frustums and that the sample points
                lie along the correct ray directions, which helps catch subtle bugs in the camera and ray construction.
            </p>
    
            <div class="image-row">
                <figure>
                    <img src="media/Part2.3_1.png" alt="3D visualization of cameras, rays, and sample points">
                    <figcaption>
                        3D visualization of the cameras (frustums), a subset of sampled rays (up to 100 rays),
                        and the corresponding sample points along each ray at a single training step.
                    </figcaption>
                </figure>
                <figure>
                      <img src="media/Part2.3_2.png" alt="3D visualization of cameras, rays, and sample points">
                      <figcaption>
                          rays sampled only from one camera
                      </figcaption>
                  </figure>
            </div>
        </section>
    
        <!-- ======================= Part 2.4 ======================= -->
        <section id="part2-4">
            <h3>Part 2.4: Neural Radiance Field Network</h3>
            <p>
                The Neural Radiance Field (NeRF) network generalizes the 2D neural field from Part 1 to model a 3D
                scene with view-dependent appearance. Instead of taking 2D pixel coordinates, the NeRF network takes
                3D world coordinates and a viewing direction as input and outputs both color and density at each 3D
                location along a ray.
            </p>
    
            <h4>Input Encoding</h4>
            <p>
                I use sinusoidal positional encoding (PE) for both the 3D position and the 3D ray direction:
            </p>
            <ul>
                <li>
                    <b>Position encoding:</b> a 3D point <code>x</code> is encoded with <code>L<sub>x</sub> = 12</code>
                    frequency bands, giving <code>3 &times; (2L<sub>x</sub> + 1)</code> channels.
                </li>
                <li>
                    <b>Direction encoding:</b> the ray direction <code>d</code> is encoded with
                    <code>L<sub>d</sub> = 6</code> frequency bands, giving <code>3 &times; (2L<sub>d</sub> + 1)</code>
                    channels.
                </li>
            </ul>
            <p>
                This positional encoding allows the network to represent high-frequency detail in both geometry and
                view-dependent color while still using a standard MLP.
            </p>
    
            <h4>Network Architecture</h4>
            <p>
                The NeRF MLP is deeper and wider than the 2D network in Part 1 to handle the increased complexity of
                a full 3D scene. My architecture is:
            </p>
            <ul>
                <li>
                    A stack of fully-connected layers that process the encoded 3D position. I use 8 layers with
                    hidden width 256 and ReLU activations.
                </li>
                <li>
                    A skip connection that concatenates the original position encoding back into the network at a
                    middle layer (after 4 layers). This helps prevent deep layers from “forgetting” the input
                    coordinates.
                </li>
                <li>
                    A density (sigma) head that maps the final position features to a single scalar density value.
                    I apply a ReLU to ensure that density is non-negative.
                </li>
                <li>
                    A feature head that produces a 256-dimensional feature vector, which is fed into the color branch.
                </li>
                <li>
                    A color branch that concatenates the feature vector with the encoded viewing direction and passes
                    them through additional layers to produce an RGB color. I apply a Sigmoid activation at the end to
                    constrain the RGB values to <code>[0, 1]</code>.
                </li>
            </ul>
            <p>
                Both the Lego and test novel views use the exact same architecture: positional encoding with
                <code>L<sub>x</sub> = 12</code> and <code>L<sub>d</sub> = 6</code>, hidden width 256, and 8 layers with
                a single skip connection in the middle. The network is implemented in PyTorch.
            </p>
    
            <div class="image-row">
                <figure>
                    <img src="media/NN.png" alt="Diagram of the NeRF MLP architecture">
                    <figcaption>
                        NeRF network architecture. The encoded 3D position is processed by a deep MLP with a skip
                        connection. The network branches into a density head and a feature head. The feature head
                        is combined with an encoded viewing direction to produce view-dependent RGB colors.
                    </figcaption>
                </figure>
            </div>
        </section>
    
        <!-- ======================= Part 2.5 ======================= -->
        <section id="part2-5">
            <h3>Part 2.5: Volume Rendering and Training</h3>
            <p>
                With the NeRF network and sampling routines in place, the final step is to implement differentiable
                volume rendering along each ray and train the network on the multi-view Lego dataset.
            </p>
    
            <h4>Volume Rendering</h4>
            <p>
                For each ray, the NeRF network predicts a density <code>&sigma;<sub>i</sub></code> and color
                <code>c<sub>i</sub></code> at a set of discrete sample points along the ray. I implemented the discrete
                volume rendering equation:
            </p>
            <p class="equation">
                Ĉ(r) = &sum;<sub>i</sub> T<sub>i</sub> &alpha;<sub>i</sub> c<sub>i</sub>, &nbsp;&nbsp;
                &alpha;<sub>i</sub> = 1 - exp(-&sigma;<sub>i</sub> &Delta;<sub>i</sub>), &nbsp;&nbsp;
                T<sub>i</sub> = exp(-&sum;<sub>j &lt; i</sub> &sigma;<sub>j</sub> &Delta;<sub>j</sub>),
            </p>
            <p>
                where <code>&Delta;<sub>i</sub></code> is the distance between neighboring samples (the step size).
                In code, I use PyTorch tensor operations and <code>torch.cumsum</code> to compute the accumulated
                transmittance and weights in a fully vectorized way, and then perform a weighted sum of the colors
                along each ray. This function is differentiable and used inside the training loop so that gradients
                flow back into the NeRF MLP.
            </p>
    
            <h4>Training Procedure</h4>
            <p>
                For each training iteration, I:
            </p>
            <ol>
                <li>Sample <code>N</code> rays (origins, directions, and target RGBs) from the dataloader using the grouped ray sampler (<code>M = 8</code> cameras per batch).</li>
                <li>Use <code>sample_along_rays</code> with <code>n_samples = 64</code>, <code>near = 2.0</code> and <code>far = 6.0</code> and <code>perturb=True</code> to obtain 3D sample points along each ray.</li>
                <li>Pass the sample points and directions through the NeRF network to obtain per-sample colors and densities.</li>
                <li>Apply the volume rendering function to collapse the samples back into a single RGB color for each ray.</li>
                <li>Compute the MSE loss between the rendered RGBs and the ground-truth image colors, and backpropagate.</li>
            </ol>
            <p>
                I use the Adam optimizer with a learning rate of <code>5 &times; 10<sup>-4</sup></code> and a batch size
                of 8192 rays per iteration. The positional encoding frequencies are fixed to
                <code>L<sub>x</sub> = 10</code> for positions and <code>L<sub>d</sub> = 4</code> for directions. I
                train the network for up to 2000 iterations, periodically evaluating on the validation set by rendering
                all 6 validation cameras and computing the average PSNR.
            </p>
    
            <h4>Training Progression and Validation PSNR</h4>

            <p>
                To better understand the perceptual quality, I also render the same validation viewpoint at different
                training iterations. The 2&times;4 grid below shows how the model evolves from a blurry, noisy estimate
                to a sharp reconstruction with clear geometry and textures.
            </p>

            <!-- 2x3 grid of intermediate validation renders -->
            <div class="grid-2x4"
                 style="
                    display:grid;
                    grid-template-columns: repeat(4, minmax(0, 1fr));
                    grid-auto-rows: auto;
                    gap:16px;
                    justify-items:center;
                    align-items:center;
                 ">
                <figure>
                    <img src="media/Part2.5_11.png" alt="Predicted validation view at early training (iteration 1)"
                         style="width:100%; max-width:260px; height:auto;">
                    <figcaption>Iteration 1</figcaption>
                </figure>
                <figure>
                    <img src="media/Part2.5_12.png" alt="Predicted validation view at intermediate training (iteration 50)"
                         style="width:100%; max-width:260px; height:auto;">
                    <figcaption>Iteration 50</figcaption>
                </figure>
                <figure>
                    <img src="media/Part2.5_13.png" alt="Predicted validation view at later training (iteration 100)"
                         style="width:100%; max-width:260px; height:auto;">
                    <figcaption>Iteration 100</figcaption>
                </figure>
                <figure>
                    <img src="media/Part2.5_14.png" alt="Predicted validation view at iteration 200"
                         style="width:100%; max-width:260px; height:auto;">
                    <figcaption>Iteration 200</figcaption>
                </figure>
                <figure>
                    <img src="media/Part2.5_15.png" alt="Predicted validation view at iteration 500"
                         style="width:100%; max-width:260px; height:auto;">
                    <figcaption>Iteration 500</figcaption>
                </figure>
                <figure>
                    <img src="media/Part2.5_21.png" alt="Predicted validation view at iteration 1000"
                         style="width:100%; max-width:260px; height:auto;">
                    <figcaption>Iteration 1000</figcaption>
                </figure>
                <figure>
                    <img src="media/Part2.5_17.png" alt="Predicted validation view at iteration 1000"
                         style="width:100%; max-width:260px; height:auto;">
                    <figcaption>Iteration 1500</figcaption>
                </figure>
                <figure>
                    <img src="media/Part2.5_18.png" alt="Predicted validation view at iteration 1000"
                         style="width:100%; max-width:260px; height:auto;">
                    <figcaption>Iteration 2000</figcaption>
                </figure>
            </div>

            <p>
                The figures below summarize how the network improves over time. I first show the PSNR curves for both
                the training batches and the held-out validation views, and then visualize snapshots of a single
                validation camera across different iterations to illustrate how the reconstruction sharpens over time.
            </p>

            <!-- PSNR plots: train and validation side by side -->
            <div class="psnr-row" style="display:flex; gap:24px; justify-content:center; align-items:flex-start; flex-wrap:wrap;">
                <figure style="flex:1 1 280px; max-width:480px; text-align:center;">
                    <img src="media/Part2.5_19.png" alt="Train PSNR over training steps" style="width:100%; height:auto;">
                    <figcaption>
                        Train PSNR over steps. The PSNR of the mini-batches gradually increases as the NeRF fits the
                        Lego radiance field.
                    </figcaption>
                </figure>
                <figure style="flex:1 1 280px; max-width:480px; text-align:center;">
                    <img src="media/Part2.5_20.png" alt="Validation PSNR over training iterations" style="width:100%; height:auto;">
                    <figcaption>
                        Validation PSNR over training. The curve tracks the average PSNR across the 6 validation views
                        and plateaus as the model converges.
                    </figcaption>
                </figure>
            </div>

    
            <h4>Novel View Spherical Rendering</h4>
            <p>
                After training, I use the provided test camera extrinsics <code>c2ws_test</code> to render a spherical
                novel-view video of the Lego scene. For each test camera pose, I render a full image using
                the <code>render_image</code> function and then concatenate the frames into a video. This produces a
                smooth fly-around of the reconstructed Lego, demonstrating that the NeRF has learned a coherent
                3D representation that generalizes to unseen viewpoints.
            </p>
    
            <div class="image-row">
                <figure>
                    <img src="media/lego_spherical_test_new.gif"
                         alt="Spherical novel-view rendering of the Lego"
                         width="320">
                    <figcaption>
                        Spherical novel-view rendering of the Lego using the test camera poses <code>c2ws_test</code>.
                        The video smoothly orbits the scene, showing consistent geometry and view-dependent shading.
                    </figcaption>
                </figure>
            </div>
        </section>

        <!-- ======================= Part 2.6 ======================= -->
        <section id="part2-6">
            <h3>Part 2.6: NeRF on My Own Object</h3>

            <p>
                In this part I applied my NeRF implementation to the real-world dataset that I captured in
                Part&nbsp;0 (a metal travel mug on a sheet of paper with an ArUco marker).
                The goal was to train a NeRF that can reproduce the training views and then render a
                novel-view gif of a virtual camera circling the object.
            </p>

            <h4>Training Setup</h4>
            <p>
                I reused the same NeRF architecture as in the Lego experiment (8-layer MLP with positional
                encoding on positions and directions), but had to make several changes to make training
                work on the real data:
            </p>
            <ul>
                <li>
                    <strong>Intrinsics and poses.</strong> I loaded the calibrated intrinsics
                    <code>K_custom</code> and camera-to-world matrices <code>c2ws_train_custom</code>,
                    <code>c2ws_val_custom</code>, and <code>c2ws_test_custom</code> from
                    <code>my_data.npz</code>. The intrinsics were built at the same resolution as
                    the training images.
                </li>
                <li>
                    <strong>Near / far bounds.</strong> Instead of the Lego values
                    (<code>near = 2.0</code>, <code>far = 6.0</code>), I used
                    <code>near = 0.12</code> and <code>far = 0.80</code>, which empirically
                    covered the depth range from the marker paper up to the front of the mug.
                </li>
                <li>
                    <strong>Samples per ray.</strong> I first debugged using
                    <code>n_samples = 32</code> and then switched to <code>n_samples = 64</code>
                    for the final training to improve visual quality, at the cost of longer training time.
                </li>
                <li>
                    <strong>Ray sampling.</strong> As in Part&nbsp;2.5, I used the grouped
                    sampler that first chooses <code>M = 8</code> cameras per batch and then
                    samples pixels within those cameras. This improves stability on multi-view
                    data.
                </li>
                <li>
                    <strong>Optimization.</strong> I used Adam with a base learning rate of
                    <code>2 &times; 10<sup>-4</sup></code>, batch size 8192 rays, and trained for
                    a few thousand steps. The learning rate was decayed by factors of 0.5 and 0.25
                    at roughly half and 80% of the total iterations, respectively.
                </li>
            </ul>

            <div class="image-grid-2">
              <figure>
                  <!-- Training loss (MSE) curve -->
                  <img src="media/Part2.6_11.png" alt="Training loss (MSE) on the custom object">
                  <figcaption>
                      Training loss (MSE) over iterations for the custom-object NeRF. The loss decreases
                      as the network overfits to all captured views.
                  </figcaption>
              </figure>
              <figure>
                  <!-- Training PSNR curve -->
                  <img src="media/Part2.6_12.png" alt="Training PSNR on the custom object">
                  <figcaption>
                      Training batch PSNR over iterations. The PSNR gradually rises into the
                      19–21&nbsp;dB range as the model fits the dataset.
                  </figcaption>
              </figure>
            </div>


            <h4>Intermediate Renders During Training</h4>
            <p>
                To visualize the learning dynamics, I periodically rendered the first validation view
                at multiple checkpoints (e.g., steps 1, 50, 100, 200, 400, 800, 1200, 1600, 2000, 2500).
                Initially the NeRF predicts nearly uniform colors, then gradually discovers the rough
                silhouette of the mug and the paper, and finally recovers some of the color contrast
                and shading, although the result remains relatively blurry compared to the ground truth.
            </p>

            <div class="grid-2x4"
                 style="
                    display:grid;
                    grid-template-columns: repeat(4, minmax(0, 1fr));
                    grid-auto-rows: auto;
                    gap:16px;
                    justify-items:center;
                    align-items:center;
                 ">
                <figure>
                    <img src="media/inter_1.png" alt="Render at iteration 1">
                    <figcaption>Iteration 1</figcaption>
                </figure>
            
                <figure>
                    <img src="media/inter_100.png" alt="Render at iteration 100">
                    <figcaption>Iteration 100</figcaption>
                </figure>
            
                <figure>
                    <img src="media/inter_200.png" alt="Render at iteration 200">
                    <figcaption>Iteration 200</figcaption>
                </figure>
            
                <figure>
                    <img src="media/inter_400.png" alt="Render at iteration 400">
                    <figcaption>Iteration 400</figcaption>
                </figure>
            
                <figure>
                    <img src="media/inter_800.png" alt="Render at iteration 800">
                    <figcaption>Iteration 800</figcaption>
                </figure>
            
                <figure>
                    <img src="media/inter_1600.png" alt="Render at iteration 1600">
                    <figcaption>Iteration 1600</figcaption>
                </figure>
            
                <figure>
                    <img src="media/inter_2000.png" alt="Render at iteration 2000">
                    <figcaption>Iteration 2000</figcaption>
                </figure>
            
                <figure>
                    <img src="media/inter_3000.png" alt="Render at iteration 3000">
                    <figcaption>Iteration 3000</figcaption>
                </figure>
            
            </div>


            <h4>Sanity Checks on 2-View and 4-View Subsets</h4>
            <p>
                Before training on the full dataset, I debugged the pipeline using small subsets of
                the training views. I first trained a NeRF on only <strong>2 views</strong>, and
                then on <strong>4 views</strong>. In both cases, the network was able to
                overfit these views reasonably well:
            </p>
            <ul>
                <li>
                    For 2 views, the reconstructions reached &gt;20&nbsp;dB PSNR on those two images,
                    and the mug, lid, and marker pattern were clearly recognizable.
                </li>
                <li>
                    For 4 views, the PSNR was slightly lower per view, but the NeRF still produced
                    plausible reconstructions for all four cameras.
                </li>
            </ul>

            <div class="image-row">
                <figure>
                    <!-- Replace with a 2-view debug panel: GT vs Pred -->
                    <img src="media/check1.png" alt="2-view NeRF debug: ground truth and predictions (200 steps)">
                </figure>
            </div>

            <div class="image-row">
                <figure>
                    <!-- Replace with a 4-view debug panel: GT vs Pred -->
                    <img src="media/check2.png" alt="2-view NeRF debug: ground truth and predictions (400 steps)">
                    <figcaption>
                        Two-view sanity check. Left: ground-truth training views. Right: NeRF predictions
                        after training only on these two images. The model overfits these views and
                        reproduces the mug and paper with reasonable sharpness.
                    </figcaption>
                </figure>
            </div>

            <h4>Novel-View Spherical Rendering (Custom Object Gif)</h4>
            <p>
                After training on the full set of images, I generated a gif of the camera circling
                the object using the starter-code pattern provided in the assignment. I first chose
                a reasonable starting camera position <code>START_POS</code> (similar in distance
                and height to one of the training views), constructed a camera-to-world matrix that
                looks at the origin via <code>look_at_origin(START_POS)</code>, and then applied a
                rotation around the vertical axis:
            </p> 

            <p>
                The resulting frames were converted to an animated gif that loops continuously:
            </p>

            <div class="image-row">
                <figure>
                    <!-- Replace src with your final gif path -->
                    <img class="small-gif"
                         src="media/final.gif"
                         alt="Novel-view rendering of the reconstructed custom object">
                    <figcaption>
                        Novel-view gif of the reconstructed mug. The virtual camera circles the object using
                        the assignment-provided <code>look_at_origin</code> and <code>rot_x</code> pattern.
                        The overall shape of the mug is visible, but the reconstruction is noticeably
                        blurry and exhibits streaking artifacts.
                    </figcaption>
                </figure>
            </div>


            <h4>Why the Result Is Not Perfect</h4>
            <p>
                Compared to the Lego experiment, the final NeRF of my custom object is much less sharp,
                and many views suffer from blur and ghosting. Several diagnostics support the idea that
                this is largely due to issues with the captured data and calibration, rather than just
                model capacity:
            </p>
            <ul>
                <li>
                    <strong>Per-view PSNR is very uneven.</strong> When I evaluated PSNR on each individual
                    training view, a few cameras reached around 20&nbsp;dB, but many others were stuck
                    around 9–12&nbsp;dB. This suggests that different views are not fully consistent with
                    a single static 3D scene, or that some poses are significantly noisier than others.
                </li>
                <li>
                    <strong>2-view / 4-view subsets work much better.</strong> On a carefully chosen
                    subset of 2 or 4 neighboring views, the NeRF overfits well and produces clean
                    reconstructions. As soon as I include many more views distributed around the mug,
                    the reconstruction degrades and becomes streaky. This pattern is consistent with
                    small calibration or pose errors that accumulate around the circle.
                </li>
                <li>
                    <strong>Real capture imperfections.</strong> During data collection, the mug and the
                    ArUco marker might have slightly moved between shots, the paper might not have been
                    perfectly flat, and the camera center-to-marker distance and orientation might have
                    varied more than desired. All of these violate the static, perfectly calibrated
                    multi-view assumption of NeRF.
                </li>
                <li>
                    <strong>Limited views and background clutter.</strong> Compared to the synthetic Lego
                    dataset, I have fewer views and a larger share of each image is background (table, wall,
                    etc.) instead of the mug itself. The network spends many samples modeling the
                    slowly varying background, which also contributes to the blurred look.
                </li>
            </ul>

            <p>
                Overall, the experiments show that the NeRF implementation itself is functioning
                correctly (as demonstrated by the smaller 2-view and 4-view experiments), but that
                achieving a high-quality reconstruction on real data is quite sensitive to the
                calibration accuracy, capture setup, and number and distribution of views. With
                more careful data capture (denser views around the object, more stable tripod,
                flatter marker sheet, and possibly cropping the images to focus more tightly on
                the mug), the quality of the custom-object NeRF would likely improve significantly.
            </p>
        </section>






    
    


    <!-- Future sections placeholders (do not render yet) -->
    <!--
    <section id="part1"><h2>Part 1 — NeRF Training</h2></section>
    <section id="part2"><h2>Part 2 — Improvements & Novel Views</h2></section>
    -->
  </main>

  <footer>
    © <script>document.write(new Date().getFullYear());</script> Project 4. All images and results by the author.
  </footer>
</body>
</html>

