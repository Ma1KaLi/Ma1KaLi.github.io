<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Project 5 — Fun With Diffusion Models!</title>
  <style>
    :root { --maxw: 980px; }
    body {
      font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif;
      margin: 0; line-height: 1.6; color: #222; background: #fafafa;
    }
    header, main, footer { max-width: var(--maxw); margin: 0 auto; padding: 20px; }
    header h1 { margin: 0 0 6px; }
    header a { color: #0b5fff; text-decoration: none; }
    .back { display: inline-block; margin-top: 6px; }

    section {
      background: #fff; border: 1px solid #eee; border-radius: 14px;
      padding: 18px; margin: 18px 0 24px; box-shadow: 0 1px 3px rgba(0,0,0,.05);
    }
    h2 { margin: 0 0 12px; font-size: 1.25rem; }
    h3 { margin-top: 12px; }
    p { margin: 8px 0; }

    /* Grids */
    .grid-2 { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 12px; }
    .grid-3 { display: grid; grid-template-columns: repeat(3, minmax(0, 1fr)); gap: 12px; }
    .grid-4 { display: grid; grid-template-columns: repeat(4, minmax(0, 1fr)); gap: 12px; }
    .grid-6 { display: grid; grid-template-columns: repeat(6, minmax(0, 1fr)); gap: 10px; }
    .image-row { display: flex; flex-wrap: wrap; gap: 14px; justify-content: center; }
    @media (max-width: 900px) {
      .grid-3, .grid-4, .grid-6 { grid-template-columns: repeat(2, minmax(0, 1fr)); }
    }
    @media (max-width: 720px) {
      .grid-2, .grid-3, .grid-4, .grid-6 { grid-template-columns: 1fr; }
      .image-row { flex-direction: column; align-items: stretch; }
    }

    figure { margin: 0; border: 1px solid #eee; border-radius: 12px; overflow: hidden; background: #fff; }
    figure img, figure video { display: block; width: 100%; height: auto; }
    figcaption { font-size: .9rem; padding: 10px 12px; background: #f6f7fb; border-top: 1px solid #eee; }

    .answer {
      background: #f4fbf6; border: 1px solid #d8f1df; border-radius: 10px;
      padding: 10px 12px; margin-top: 10px;
    }
    .note {
      font-size: .92rem; color: #555; background: #fff7e6;
      border: 1px solid #ffe2a8; border-radius: 10px;
      padding: 10px 12px; margin-top: 10px;
    }

    /* Tables (if needed later) */
    table { width: 100%; border-collapse: collapse; }
    thead th { text-align: left; font-weight: 600; padding: 10px 10px; background: #f6f7fb; border-bottom: 1px solid #eee; }
    tbody td { padding: 10px; border-top: 1px solid #eee; vertical-align: middle; }
    td img { width: 240px; height: auto; display: block; border-radius: 10px; border: 1px solid #eee; }
    td.number, td.time { white-space: nowrap; font-variant-numeric: tabular-nums; }
    @media (max-width: 720px) { td img { width: 100%; } }

    footer { font-size: .9rem; color: #666; }
  </style>
</head>
<body>
  <header>
    <h1>Project 5 — Fun With Diffusion Models!</h1>
    <a class="back" href="../index.html">← Back to main portfolio</a>
  </header>

  <main>

    <!-- ======================= Part A ======================= -->
    <section id="partA">
      <h2>Part A — The Power of Diffusion Models!</h2>

      <!-- Part 0: Setup -->
      <section id="part0">
        <h2>Part 0 — Setup</h2>

        <p>
          I use the two–stage <strong>DeepFloyd IF</strong> model from HuggingFace: a 64×64 base
          text-to-image diffusion model (stage&nbsp;1) followed by a 256×256 super-resolution
          model (stage&nbsp;2). All experiments in Part&nbsp;A are run in a Colab notebook with
          a single GPU and a fixed random seed of <strong>100</strong>.
        </p>

        <div class="answer">
          <strong>Text prompts used in Part A</strong>
          <ul style="margin:6px 0 0 18px;">
            <li><b>Prompt 1:</b> <em>The black hole and the spacecraft in the film Interstellar, original cinematic visual style</em></li>
            <li><b>Prompt 2:</b> <em>wide view of a busy city street in daytime, realistic photo</em></li>
            <li><b>Prompt 3:</b> <em>close-up portrait of an elderly scientist with wild white hair, black-and-white photo</em></li>
          </ul>
          <p style="margin-top:6px;">
            Each prompt is encoded once using the provided T5 encoder space and cached in
            <code>prompt_embeds_dict.pth</code>. I reuse these embeddings throughout Part&nbsp;A.
          </p>
        </div>

        <p>
          To understand the baseline behavior of DeepFloyd IF, I generate images for the three
          prompts above using two different sampling budgets:
          <code>num_inference_steps = 20</code> and <code>200</code>. The grid below shows the
          stage&nbsp;2 outputs; the seed is fixed so that only the number of denoising steps
          changes.
        </p>

        <!-- 3×2 grid: prompts × inference steps -->
        <div class="grid-3">
          <figure>
            <img src="media/part0_1.png" alt="Prompt 1, 20 steps">
            <figcaption>Prompt 1, 20 inference steps</figcaption>
          </figure>
          <figure>
            <img src="media/part0_2.png" alt="Prompt 2, 20 steps">
            <figcaption>Prompt 2, 20 inference steps</figcaption>
          </figure>
          <figure>
            <img src="media/part0_3.png" alt="Prompt 3, 20 steps">
            <figcaption>Prompt 3, 20 inference steps</figcaption>
          </figure>

          <figure>
            <img src="media/part0_4.png" alt="Prompt 1, 200 steps">
            <figcaption>Prompt 1, 200 inference steps</figcaption>
          </figure>
          <figure>
            <img src="media/part0_5.png" alt="Prompt 2, 200 steps">
            <figcaption>Prompt 2, 200 inference steps</figcaption>
          </figure>
          <figure>
            <img src="media/part0_6.png" alt="Prompt 3, 200 steps">
            <figcaption>Prompt 3, 200 inference steps</figcaption>
          </figure>
        </div>

        <div class="note">
          Increasing <code>num_inference_steps</code> from 20 to 200 generally sharpens edges and
          reduces artifacts (especially in high-frequency regions like foliage and jewelry), but
          also makes sampling an order of magnitude slower. For the rest of the project I strike a
          balance by using a moderate number of steps together with improved guidance techniques.
        </div>
      </section>


      <!-- ======================= Part 1: Sampling Loops ======================= -->
      <section id="part1">
        <h2>Part 1 — Sampling Loops</h2>

        <!-- 1.1 Forward -->
        <h3 id="part1-1">1.1 Implementing the Forward Process</h3>
        <p>
          A diffusion model defines a <em>forward process</em> that gradually corrupts a clean image
          <code>x_0</code> into noisy versions <code>x_t</code>. Using the schedule
          <code>&#257;&#945;<sub>t</sub></code> provided by the model, I implemented:
        </p>
        <p class="answer">
          <code>x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * eps</code>, with
          <code>eps ~ N(0, I)</code>.
        </p>
        <p>
          The Campanile image from the starter code serves as my test case. Below I show
          <code>t = 250, 500, 750</code> using the <code>forward</code> function.
        </p>
        <div class="grid-4">
          <figure>
            <img src="media/part1_1_4.png" alt="Clean campanile">
            <figcaption><code>t = 0</code> (clean image).</figcaption>
          </figure>
          <figure>
            <img src="media/part1_1_1.png" alt="Campanile t=250">
            <figcaption><code>t = 250</code> — noticeable grain but structure still clear.</figcaption>
          </figure>
          <figure>
            <img src="media/part1_1_2.png" alt="Campanile t=500">
            <figcaption><code>t = 500</code> — tower barely visible under strong noise.</figcaption>
          </figure>
          <figure>
            <img src="media/part1_1_3.png" alt="Campanile t=750">
            <figcaption><code>t = 750</code> — almost pure Gaussian noise.</figcaption>
          </figure>
        </div>
        <div class="note">
          As <code>t</code> increases, the effective signal-to-noise ratio collapses; by
          <code>t = 750</code> it is hard to tell that the image ever contained a tower at all.
          The reverse process has to recover global structure from almost no obvious signal.
        </div>

        <!-- 1.2 Classical Denoising -->
        <h3 id="part1-2">1.2 Classical Denoising</h3>
        <p>
          Before relying on learned models, I tried denoising the same noisy Campanile with
          a simple <strong>Gaussian blur</strong>. The top row shows the noisy images at
          timesteps <code>t = 250, 500, 750</code>, and the bottom row shows the corresponding
          Gaussian-blurred results.
        </p>

        <div class="grid-3">
          <!-- Noisy images -->
          <figure>
            <img src="media/part1_1_1.png" alt="Noisy Campanile t=250">
            <figcaption>Noisy Campanile at <code>t = 250</code></figcaption>
          </figure>
          <figure>
            <img src="media/part1_1_2.png" alt="Noisy Campanile t=500">
            <figcaption>Noisy Campanile at <code>t = 500</code></figcaption>
          </figure>
          <figure>
            <img src="media/part1_1_3.png" alt="Noisy Campanile t=750">
            <figcaption>Noisy Campanile at <code>t = 750</code></figcaption>
          </figure>

          <!-- Gaussian-blurred images -->
          <figure>
            <img src="media/part1_2_1.png" alt="Gaussian blur t=250">
            <figcaption>Gaussian Blur Denoising at <code>t = 250</code></figcaption>
          </figure>
          <figure>
            <img src="media/part1_2_2.png" alt="Gaussian blur t=500">
            <figcaption>Gaussian Blur Denoising at <code>t = 500</code></figcaption>
          </figure>
          <figure>
            <img src="media/part1_2_3.png" alt="Gaussian blur t=750">
            <figcaption>Gaussian Blur Denoising at <code>t = 750</code></figcaption>
          </figure>
        </div>

        <div class="note">
          Blurring removes a fair amount of high-frequency noise, but even at the lowest
          noise level the tower becomes visibly softer, and at <code>t = 750</code> the
          result is just a smooth color field with almost no recognizable structure.
        </div>


        <!-- 1.3 One-Step Denoising -->
        <h3 id="part1-3">1.3 One-Step Denoising</h3>
        <p>
          Gaussian blurring clearly cannot fully undo the heavy corruption in the forward process.
          Next, I apply a single UNet denoising step to predict the noise and recover an estimate
          of the clean image. This uses the formula
          <code>x0_hat = (x_t - sqrt(1 - alpha_bar_t) * eps_hat) / sqrt(alpha_bar_t)</code>.
        </p>

        <div class="image-row">
          <!-- Group 1: t = 250 -->
          <figure style="flex:1 1 280px; max-width:340px; text-align:center;">
            <div class="grid-3" style="grid-template-columns:repeat(3,minmax(0,1fr)); gap:8px; margin-bottom:6px;">
              <div>
                <div style="font-size:0.9rem; margin-bottom:4px;">Original</div>
                <img src="media/part1_3_clean_t_250.png" alt="Original t=250">
              </div>
              <div>
                <div style="font-size:0.9rem; margin-bottom:4px;">Noisy (t = 250)</div>
                <img src="media/part1_3_noisy_t_250.png" alt="Noisy t=250">
              </div>
              <div>
                <div style="font-size:0.9rem; margin-bottom:4px;">One-Step Denoised</div>
                <img src="media/part1_3_denoised_t_250.png" alt="One-step denoised t=250">
              </div>
            </div>
            <figcaption>One-Step Denoised Image 1 (t = 250)</figcaption>
          </figure>

          <!-- Group 2: t = 500 -->
          <figure style="flex:1 1 280px; max-width:340px; text-align:center;">
            <div class="grid-3" style="grid-template-columns:repeat(3,minmax(0,1fr)); gap:8px; margin-bottom:6px;">
              <div>
                <div style="font-size:0.9rem; margin-bottom:4px;">Original</div>
                <img src="media/part1_3_clean_t_500.png" alt="Original t=500">
              </div>
              <div>
                <div style="font-size:0.9rem; margin-bottom:4px;">Noisy (t = 500)</div>
                <img src="media/part1_3_noisy_t_500.png" alt="Noisy t=500">
              </div>
              <div>
                <div style="font-size:0.9rem; margin-bottom:4px;">One-Step Denoised</div>
                <img src="media/part1_3_denoised_t_500.png" alt="One-step denoised t=500">
              </div>
            </div>
            <figcaption>One-Step Denoised Image 2 (t = 500)</figcaption>
          </figure>

          <!-- Group 3: t = 750 -->
          <figure style="flex:1 1 280px; max-width:340px; text-align:center;">
            <div class="grid-3" style="grid-template-columns:repeat(3,minmax(0,1fr)); gap:8px; margin-bottom:6px;">
              <div>
                <div style="font-size:0.9rem; margin-bottom:4px;">Original</div>
                <img src="media/part1_3_clean_t_750.png" alt="Original t=750">
              </div>
              <div>
                <div style="font-size:0.9rem; margin-bottom:4px;">Noisy (t = 750)</div>
                <img src="media/part1_3_noisy_t_750.png" alt="Noisy t=750">
              </div>
              <div>
                <div style="font-size:0.9rem; margin-bottom:4px;">One-Step Denoised</div>
                <img src="media/part1_3_denoised_t_750.png" alt="One-step denoised t=750">
              </div>
            </div>
            <figcaption>One-Step Denoised Image 3 (t = 750)</figcaption>
          </figure>
        </div>

        <div class="note">
          For mild noise (<code>t = 250</code>) a single UNet pass already reconstructs a
          fairly sharp tower, while at <code>t = 750</code> the model can only guess the
          rough silhouette. This motivates the iterative sampling loop used in the next section.
        </div>


        <!-- 1.4 Iterative Denoising -->
        <h3 id="part1-4">1.4 Iterative Denoising</h3>
        <p>
          To better exploit the diffusion model, I run denoising <em>iteratively</em> along a
          strided schedule of timesteps. Starting from a noisy image at some <code>t</code>, I
          repeatedly estimate <code>x_0</code> with the UNet and use the DDPM update to move to a
          slightly less noisy sample <code>x_{t'}</code>. This gradually brings the sample back to
          the data manifold.
        </p>
        <p>
          The top row below shows the Campanile at every 5th step of the iterative loop
          (corresponding to timesteps <code>t = 90, 240, 390, 540, 690</code>). The bottom row
          compares three different ways of “denoising” the same starting point.
        </p>

        <!-- Top row: noisy images every 5th loop -->
        <div class="image-row">
          <figure>
            <img src="media/part1_4_t_90.png" alt="Noisy Campanile t=90">
            <figcaption>Noisy Campanile at <code>t = 90</code></figcaption>
          </figure>
          <figure>
            <img src="media/part1_4_t_240.png" alt="Noisy Campanile t=240">
            <figcaption>Noisy Campanile at <code>t = 240</code></figcaption>
          </figure>
          <figure>
            <img src="media/part1_4_t_390.png" alt="Noisy Campanile t=390">
            <figcaption>Noisy Campanile at <code>t = 390</code></figcaption>
          </figure>
          <figure>
            <img src="media/part1_4_t_540.png" alt="Noisy Campanile t=540">
            <figcaption>Noisy Campanile at <code>t = 540</code></figcaption>
          </figure>
          <figure>
            <img src="media/part1_4_t_690.png" alt="Noisy Campanile t=690">
            <figcaption>Noisy Campanile at <code>t = 690</code></figcaption>
          </figure>
        </div>

        <!-- Bottom row: original vs three denoising methods -->
        <div class="image-row" style="margin-top:18px;">
          <figure>
            <img src="media/part1_4_original.png" alt="Original Campanile">
            <figcaption>Original</figcaption>
          </figure>
          <figure>
            <img src="media/part1_4_iterative_denoising.png" alt="Iteratively denoised Campanile">
            <figcaption>Iteratively Denoised Campanile</figcaption>
          </figure>
          <figure>
            <img src="media/part1_4_onestep_denoising.png" alt="One-step denoised Campanile">
            <figcaption>One-Step Denoised Campanile</figcaption>
          </figure>
          <figure>
            <img src="media/part1_4_gaussian_blur.png" alt="Gaussian blurred Campanile">
            <figcaption>Gaussian Blurred Campanile</figcaption>
          </figure>
        </div>

        <div class="note">
          The iterative diffusion sampler recovers a much crisper tower and sky than both the
          single-step estimate and the Gaussian blur. The one-step result still looks slightly
          washed out, whereas the Gaussian blur completely destroys fine structure.
        </div>


        <!-- 1.5 Sampling -->
        <h3 id="part1-5">1.5 Diffusion Model Sampling</h3>
        <p>
          With the iterative loop in place, I can now generate images from scratch by starting
          from Gaussian noise and setting <code>i_start = 0</code>. Using the neutral prompt
          <code>"a high quality photo"</code>, I sample five random images from the stage&nbsp;1
          model.
        </p>
        <div class="grid-5 image-row">
          <figure style="flex:1 1 150px; max-width:180px;">
            <img src="media/part1_5_1.png" alt="Sample 1">
            <figcaption>Sample 1.</figcaption>
          </figure>
          <figure style="flex:1 1 150px; max-width:180px;">
            <img src="media/part1_5_2.png" alt="Sample 2">
            <figcaption>Sample 2.</figcaption>
          </figure>
          <figure style="flex:1 1 150px; max-width:180px;">
            <img src="media/part1_5_3.png" alt="Sample 3">
            <figcaption>Sample 3.</figcaption>
          </figure>
          <figure style="flex:1 1 150px; max-width:180px;">
            <img src="media/part1_5_4.png" alt="Sample 4">
            <figcaption>Sample 4.</figcaption>
          </figure>
          <figure style="flex:1 1 150px; max-width:180px;">
            <img src="media/part1_5_5.png" alt="Sample 5">
            <figcaption>Sample 5.</figcaption>
          </figure>
        </div>
        <div class="note">
          With only a vague “high quality photo” prompt, the model produces a wide variety of
          scenes—landscapes, portraits, and abstract patterns—illustrating both the richness and
          randomness of the learned prior.
        </div>

        <!-- 1.6 Classifier-Free Guidance (CFG) -->
        <h3 id="part1-6">1.6 Classifier-Free Guidance (CFG)</h3>
        <p>
          Classifier-free guidance sharpens samples by mixing a conditional and an unconditional
          noise estimate. Given <code>eps_c</code> (with the text prompt) and <code>eps_u</code>
          (with an empty prompt), the guided estimate is
          <code>eps = eps_u + gamma * (eps_c - eps_u)</code>. I set <code>gamma = 7</code> and
          sample from pure noise using the same seed as before.
        </p>

        <div class="image-row">
          <figure style="flex:1 1 160px; max-width:200px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">CFG sample 1</div>
            <img src="media/part1_6_1.png" alt="CFG sample 1">
          </figure>
          <figure style="flex:1 1 160px; max-width:200px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">CFG sample 2</div>
            <img src="media/part1_6_2.png" alt="CFG sample 2">
          </figure>
          <figure style="flex:1 1 160px; max-width:200px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">CFG sample 3</div>
            <img src="media/part1_6_3.png" alt="CFG sample 3">
          </figure>
          <figure style="flex:1 1 160px; max-width:200px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">CFG sample 4</div>
            <img src="media/part1_6_4.png" alt="CFG sample 4">
          </figure>
          <figure style="flex:1 1 160px; max-width:200px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">CFG sample 5</div>
            <img src="media/part1_6_5.png" alt="CFG sample 5">
          </figure>
        </div>

        <h4 style="margin-top:16px; color:#777; font-weight:500;">Images</h4>

        <div class="note">
          Compared to unguided sampling, CFG tends to produce crisper edges and more coherent
          objects (faces, silhouettes, and reflections), while still allowing some diversity
          across different random draws with the same prompt.
        </div>


        <!-- 1.7 Image-to-image Translation (SDEdit) -->
        <h3 id="part1-7">1.7 Image-to-image Translation (SDEdit)</h3>
        <p>
          SDEdit views image editing as “add noise, then project the result back to the natural
          image manifold” using the diffusion model. I first add noise to a real image at a chosen
          timestep, then run <code>iterative_denoise_cfg</code> starting from index
          <code>i_start</code> along the strided schedule. Smaller <code>i_start</code> values
          apply heavier edits, while larger values keep the original closer to intact.
        </p>

        <!-- Campanile sequence -->
        <div class="image-row">
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 1)</div>
            <img src="media/part1_7_0_campanile_1.png" alt="Campanile SDEdit i_start=1">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 3)</div>
            <img src="media/part1_7_0_campanile_2.png" alt="Campanile SDEdit i_start=3">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 5)</div>
            <img src="media/part1_7_0_campanile_3.png" alt="Campanile SDEdit i_start=5">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 7)</div>
            <img src="media/part1_7_0_campanile_4.png" alt="Campanile SDEdit i_start=7">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 10)</div>
            <img src="media/part1_7_0_campanile_5.png" alt="Campanile SDEdit i_start=10">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 20)</div>
            <img src="media/part1_7_0_campanile_6.png" alt="Campanile SDEdit i_start=20">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">original</div>
            <img src="media/part1_1_4.png" alt="Great Hall SDEdit i_start=20">
          </figure>
        </div>
        <p style="margin-top:6px; font-weight:500;">Campanile Sequence</p>

        <!-- Tiananmen sequence -->
        <div class="image-row" style="margin-top:12px;">
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 1)</div>
            <img src="media/part1_7_0_tiananmen_0.png" alt="Tiananmen SDEdit i_start=1">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 3)</div>
            <img src="media/part1_7_0_tiananmen_1.png" alt="Tiananmen SDEdit i_start=3">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 5)</div>
            <img src="media/part1_7_0_tiananmen_2.png" alt="Tiananmen SDEdit i_start=5">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 7)</div>
            <img src="media/part1_7_0_tiananmen_3.png" alt="Tiananmen SDEdit i_start=7">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 10)</div>
            <img src="media/part1_7_0_tiananmen_4.png" alt="Tiananmen SDEdit i_start=10">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 20)</div>
            <img src="media/part1_7_0_tiananmen_5.png" alt="Tiananmen SDEdit i_start=20">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">original</div>
            <img src="media/part1_7_0_tiananmen_6.png" alt="Great Hall SDEdit i_start=20">
          </figure>
        </div>
        <p style="margin-top:6px; font-weight:500;">Tiananmen Image Sequence</p>

        <!-- Great Hall sequence -->
        <div class="image-row" style="margin-top:12px;">
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 1)</div>
            <img src="media/part1_7_0_greathall_0.png" alt="Great Hall SDEdit i_start=1">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 3)</div>
            <img src="media/part1_7_0_greathall_1.png" alt="Great Hall SDEdit i_start=3">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 5)</div>
            <img src="media/part1_7_0_greathall_2.png" alt="Great Hall SDEdit i_start=5">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 7)</div>
            <img src="media/part1_7_0_greathall_3.png" alt="Great Hall SDEdit i_start=7">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 10)</div>
            <img src="media/part1_7_0_greathall_4.png" alt="Great Hall SDEdit i_start=10">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">SDEdit (i_start = 20)</div>
            <img src="media/part1_7_0_greathall_5.png" alt="Great Hall SDEdit i_start=20">
          </figure>
          <figure style="flex:1 1 150px; max-width:180px; text-align:center;">
            <div style="font-size:0.9rem; margin-bottom:4px;">original</div>
            <img src="media/part1_7_0_greathall_6.png" alt="Great Hall SDEdit i_start=20">
          </figure>
        </div>
        <p style="margin-top:6px; font-weight:500;">Great Hall Image Sequence</p>

        <div class="note">
          Moving from <code>i_start = 1</code> to <code>i_start = 20</code> within each row, the
          images gradually align more closely with the original photograph while still inheriting
          textures from the diffusion prior. For some scenes the transition between intermediate
          steps (e.g., from 3 to 5) can be surprisingly large, reflecting the model’s non-linear
          geometry in image space.
        </div>


        <!-- 1.7.1 Hand-drawn and web images -->
        <h4 id="part1-7-1">1.7.1 Editing Hand-Drawn and Web Images</h4>
        <p>
          The same SDEdit procedure is particularly fun on non-photorealistic inputs. I first
          download a cartoon-style web image, run it through <code>process_pil_im</code>, and
          then apply SDEdit with several noise levels.
        </p>
        <div class="grid-6">
          <figure>
            <img src="media/p1_7_web_sde_1.png" alt="Web image sde 1">
            <figcaption>Web image, <code>i_start = 1</code>.</figcaption>
          </figure>
          <figure>
            <img src="media/p1_7_web_sde_3.png" alt="Web image sde 3">
            <figcaption><code>i_start = 3</code>.</figcaption>
          </figure>
          <figure>
            <img src="media/p1_7_web_sde_5.png" alt="Web image sde 5">
            <figcaption><code>i_start = 5</code>.</figcaption>
          </figure>
          <figure>
            <img src="media/p1_7_web_sde_7.png" alt="Web image sde 7">
            <figcaption><code>i_start = 7</code>.</figcaption>
          </figure>
          <figure>
            <img src="media/p1_7_web_sde_10.png" alt="Web image sde 10">
            <figcaption><code>i_start = 10</code>.</figcaption>
          </figure>
          <figure>
            <img src="media/p1_7_web_sde_20.png" alt="Web image sde 20">
            <figcaption><code>i_start = 20</code>.</figcaption>
          </figure>
        </div>
        <p>
          I repeat the experiment with two quick doodles drawn in the Colab canvas—a simple
          landscape and a small character sketch. The model consistently “upgrades” flat drawings
          into textured scenes while roughly preserving the original layout.
        </p>
        <div class="grid-3">
          <figure>
            <img src="media/p1_7_draw1_sequence.png" alt="Hand drawn sequence 1">
            <figcaption>Sequence for hand-drawn image 1.</figcaption>
          </figure>
          <figure>
            <img src="media/p1_7_draw2_sequence.png" alt="Hand drawn sequence 2">
            <figcaption>Sequence for hand-drawn image 2.</figcaption>
          </figure>
          <figure>
            <img src="media/p1_7_draw_originals.png" alt="Original drawings">
            <figcaption>Original doodles for reference.</figcaption>
          </figure>
        </div>

        <!-- 1.7.2 Inpainting -->
        <h4 id="part1-7-2">1.7.2 Inpainting</h4>
        <p>
          Inpainting uses the diffusion model to hallucinate content only in a masked region.
          At each denoising step I overwrite the unmasked pixels with a noised version of the
          original image, ensuring that those locations stay anchored while the masked area
          evolves freely.
        </p>
        <div class="grid-3">
          <figure>
            <img src="media/p1_7_inpaint_mask.png" alt="Inpainting mask">
            <figcaption>Campanile with a mask over the tower top.</figcaption>
          </figure>
          <figure>
            <img src="media/p1_7_inpaint_campanile.png" alt="Inpainted campanile">
            <figcaption>One inpainted version of the Campanile.</figcaption>
          </figure>
          <figure>
            <img src="media/p1_7_inpaint_custom.png" alt="Inpainted custom image">
            <figcaption>Inpainting on a custom cityscape: replacing the sky region.</figcaption>
          </figure>
        </div>
        <div class="note">
          Because the model was never explicitly trained for inpainting, results vary between runs.
          Re-sampling a few times usually produces at least one plausible completion for the masked
          area.
        </div>

        <!-- 1.7.3 Text-conditional I2I -->
        <h4 id="part1-7-3">1.7.3 Text-Conditional Image-to-Image Translation</h4>
        <p>
          Finally, I combine SDEdit with stronger text conditioning. Instead of using the neutral
          prompt, I choose prompts such as <em>“a rocket ship launching into the sky”</em> or
          <em>“a pencil drawing on a sketchbook”</em> and run SDEdit starting from real photos.
        </p>
        <div class="grid-3">
          <figure>
            <img src="media/p1_7_text_campanile_rocket.png" alt="Rocket ship campanile">
            <figcaption>Campanile gradually morphed into a rocket-launch scene.</figcaption>
          </figure>
          <figure>
            <img src="media/p1_7_text_bigben_pencil.png" alt="Pencil big ben">
            <figcaption>A landmark translated into a pencil-style illustration.</figcaption>
          </figure>
          <figure>
            <img src="media/p1_7_text_custom_sequence.png" alt="Custom text sequence">
            <figcaption>Sequence of edits on a custom building photo.</figcaption>
          </figure>
        </div>
        <div class="note">
          The images retain the coarse layout of the original photographs (horizon line, building
          location) while the textures and colors are pulled toward the new textual description.
        </div>

        <!-- 1.8 Visual Anagrams -->
        <h3 id="part1-8">1.8 Visual Anagrams</h3>
        <p>
          Visual anagrams are optical illusions that look like one thing upright and something
          else when flipped upside down. I implement <code>make_flip_illusion</code>, which at
          each step averages two CFG noise estimates:
        </p>
        <div class="answer">
          <ul style="margin:6px 0 0 18px;">
            <li>Estimate <code>eps_1</code> on <code>x_t</code> with prompt <code>p1</code>.</li>
            <li>Flip <code>x_t</code> 180° and estimate <code>eps_2</code> with prompt <code>p2</code>.</li>
            <li>Flip <code>eps_2</code> back and set <code>eps = (eps_1 + eps_2) / 2</code>.</li>
          </ul>
        </div>
        <p>
          Starting from pure noise and using this combined noise in the DDPM update yields a single
          image whose semantics change under rotation.
        </p>
        <div class="grid-2">
          <figure>
            <img src="media/part1_8_1.png" alt="Illusion 1 upright">
            <figcaption>Illusion 1 upright: e.g., “a photo of a dress”.</figcaption>
          </figure>
          <figure>
            <img src="media/part1_8_2.png" alt="Illusion 1 flipped">
            <figcaption>Illusion 1 flipped: e.g., “a photo of an old woman”.</figcaption>
          </figure>
        </div>
        <div class="grid-2" style="margin-top:12px;">
          <figure>
            <img src="media/part1_8_3.png" alt="Illusion 2 upright">
            <figcaption>Illusion 2 upright: "a painting of a truck".</figcaption>
          </figure>
          <figure>
            <img src="media/part1_8_4.png" alt="Illusion 2 flipped">
            <figcaption>Illusion 2 flipped: "a painting of a deer".</figcaption>
          </figure>
        </div>
        <div class="note">
          These illusions are quite sensitive to random seed and guidance scale; it often takes a
          few attempts before both orientations look recognizable and match their respective prompts.
        </div>

        <!-- 1.9 Hybrid Images -->
        <h3 id="part1-9">1.9 Hybrid Images</h3>
        <p>
          Hybrid images mix low frequencies from one concept with high frequencies from another.
          I implement <code>make_hybrids</code>, which computes two CFG noise estimates for
          prompts <code>p1</code> and <code>p2</code> and combines them in frequency space:
        </p>
        <p class="answer">
          <code>eps = lowpass(eps_1) + highpass(eps_2)</code>, where lowpass is a Gaussian blur
          (kernel&nbsp;33, sigma&nbsp;2) and highpass subtracts a blurred copy from <code>eps_2</code>.
        </p>
        <p>
          The resulting image looks like <code>p1</code> from far away (because low frequencies
          dominate) but reveals details of <code>p2</code> up close.
        </p>
        <div class="grid-2">
          <figure>
            <img src="media/p1_9_hybrid1.png" alt="Hybrid 1">
            <figcaption>
              Hybrid 1: e.g., low-frequency scientist portrait + high-frequency actress portrait.
            </figcaption>
          </figure>
          <figure>
            <img src="media/p1_9_hybrid2.png" alt="Hybrid 2">
            <figcaption>
              Hybrid 2: cartoon emoji face blended with a landscape-style prompt.
            </figcaption>
          </figure>
        </div>
        <div class="note">
          Just like in the classic CS&nbsp;180 hybrid-image assignment, choosing prompts with
          complementary structure is crucial. Faces vs. faces and emoji pairs tend to work
          especially well, while highly dissimilar layouts are harder for the viewer to parse.
        </div>

      </section> <!-- end Part 1 -->
    </section> <!-- end Part A -->

    <!-- ===================== Part B Overview ===================== -->

    <h2 id="part-b">Part B: Flow Matching from Scratch!</h2>
    
    <p>
    In this part of the project, I moved away from pretrained diffusion models and
    trained several <strong>flow-based denoisers from scratch on MNIST</strong>.
    Starting from a simple one-step UNet denoiser, I gradually added time
    conditioning and class conditioning, and finally used classifier-free guidance
    to generate clean, class-controllable digits from pure Gaussian noise.
    </p>
    
    <!-- ===================== B.1.1 Implementing UNet ===================== -->
    
    <h3 id="part-b-1-1">Part B.1.1: Implementing a Single-Step Denoising UNet</h3>
    
    <p>
    I first implemented the provided UNet architecture using PyTorch. The model is
    fully convolutional and follows a standard encoder–decoder design with skip
    connections:
    three <code>DownBlock</code>s progressively reduce spatial resolution, then
    two <code>UpBlock</code>s and a final <code>ConvBlock</code> reconstruct a
    28×28 output. Each block is built from simple operations
    (<code>Conv</code>, <code>DownConv</code>, <code>UpConv</code>,
    <code>Flatten</code>, <code>Unflatten</code>, and channel-wise
    <code>Concat</code>), and uses BatchNorm + GELU activations.
    </p>
    
    <p>
    The UNet takes a noisy image as input and outputs a denoised image of the same
    shape (1×28×28). This architecture will be reused for all later experiments,
    with only the conditioning mechanism changing.
    </p>
    
    <!-- ===================== B.1.2.0 Noising Process ===================== -->
    
    <h3 id="part-b-1-2-0">Part B.1.2.0: Noising Process</h3>
    
    <p>
    To generate training pairs for the denoiser, I corrupt MNIST digits with
    additive Gaussian noise:
    </p>
    
    <p style="text-align:center;">
    <code>z = x + σ · ε</code>, where <code>ε ~ N(0, I)</code>.
    </p>
    
    <p>
    Here <code>x</code> is a clean digit in <code>[0, 1]</code>, and
    <code>σ</code> controls the noise level. I visualized the noising process for a
    fixed digit and noise levels
    <code>σ ∈ {0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0}</code>. As σ increases, the
    digit gradually disappears into Gaussian noise while the overall contrast
    remains bounded by the [0, 1] range.
    </p>
    
    <div class="image-row">
      <figure>
        <img src="media/b1_noising_grid.png" alt="MNIST digits with different noise levels sigma">
        <figcaption>
          Noising process on one MNIST digit for different values of σ. Larger σ
          produces progressively noisier images.
        </figcaption>
      </figure>
    </div>
    
    <!-- ===================== B.1.2.1 Training single-step denoiser ===================== -->
    
    <h3 id="part-b-1-2-1">Part B.1.2.1: Training the Single-Step Denoiser</h3>
    
    <p>
    Next, I trained the UNet to directly map a noisy image
    <code>z = x + 0.5 · ε</code> back to its clean target <code>x</code> using an
    L2 reconstruction loss. I used the MNIST training split, batch size 256, and
    Adam optimizer with learning rate 1e−4 for 5 epochs. At each iteration I
    sample a fresh <code>ε</code>, so the model sees a different noisy version of
    the same digit every epoch, which improves generalization.
    </p>
    
    <h4>Training Loss Curve</h4>
    
    <div class="image-row">
      <figure>
        <img src="media/b1_denoiser_train_loss.png" alt="Training loss curve for single-step denoising UNet">
        <figcaption>
          Training loss of the single-step UNet denoiser for σ = 0.5 over 5 epochs. The
          loss decreases steadily and starts to flatten as the model converges.
        </figcaption>
      </figure>
    </div>
    
    <h4>Denoising Results on Test Set</h4>
    
    <div class="image-row">
      <figure>
        <img src="media/b1_test_noisy_sigma05.png" alt="Noisy test images with sigma = 0.5">
        <figcaption>
          Test digits corrupted with Gaussian noise σ = 0.5.
        </figcaption>
      </figure>
      <figure>
        <img src="media/b1_test_denoised_epoch1.png" alt="Denoised results after 1 epoch of training">
        <figcaption>
          Denoised outputs after 1 epoch. Digits start to emerge from the noise, but
          the shapes are still blurry and edges are not very sharp.
        </figcaption>
      </figure>
      <figure>
        <img src="media/b1_test_denoised_epoch5.png" alt="Denoised results after 5 epochs of training">
        <figcaption>
          Denoised outputs after 5 epochs. The digits are much clearer and closely
          match the clean MNIST digits, with most background noise removed.
        </figcaption>
      </figure>
    </div>
    
    <!-- ===================== B.1.2.2 OOD testing ===================== -->
    
    <h3 id="part-b-1-2-2">Part B.1.2.2: Out-of-Distribution Testing</h3>
    
    <p>
    Although the denoiser was trained only on σ = 0.5, I evaluated it on the same
    test digit with a range of noise levels
    <code>σ ∈ {0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0}</code>. For each σ, I applied
    noise once and then passed the noisy image through the trained UNet.
    </p>
    
    <div class="image-row">
      <figure>
        <img src="media/b1_ood_sigma_grid.png" alt="Out-of-distribution denoising results for different sigmas">
        <figcaption>
          Out-of-distribution denoising results for a fixed digit at different noise
          levels. The model generalizes reasonably well between σ = 0.2 and 0.6, but
          struggles when σ is extremely high (0.8 or 1.0), where the input is close
          to pure noise.
        </figcaption>
      </figure>
    </div>
    
    <p>
    For small and moderate noise (up to about σ = 0.6), the model still recovers a
    recognizable digit even though it never saw those exact noise levels during
    training. When σ becomes very large, the input no longer carries enough
    information, so the UNet can only guess a vague “4-like” pattern.
    </p>
    
    <!-- ===================== B.1.2.3 Denoising pure noise ===================== -->
    
    <h3 id="part-b-1-2-3">Part B.1.2.3: Denoising Pure Noise</h3>
    
    <p>
    I then retrained the UNet on a more extreme setup: the input is <em>pure
    Gaussian noise</em> <code>ε ~ N(0, I)</code>, while the target is a clean MNIST
    digit <code>x</code>. The architecture and optimizer settings are the same as
    before. Now the model must generate a digit from a blank noisy canvas in one
    step, making the task fully generative.
    </p>
    
    <h4>Training Loss Curve</h4>
    
    <div class="image-row">
      <figure>
        <img src="media/b1_purenoise_train_loss.png" alt="Training loss curve for pure-noise denoiser">
        <figcaption>
          Training loss when denoising pure noise. The curve still decreases, but
          more slowly than in the σ = 0.5 setting, reflecting the harder task.
        </figcaption>
      </figure>
    </div>
    
    <h4>Denoising Results on Pure Noise</h4>
    
    <div class="image-row">
      <figure>
        <img src="media/b1_purenoise_input.png" alt="Pure Gaussian noise inputs">
        <figcaption>
          Pure Gaussian noise sampled as input to the denoiser.
        </figcaption>
      </figure>
      <figure>
        <img src="media/b1_purenoise_epoch1.png" alt="Pure-noise denoising results after 1 epoch">
        <figcaption>
          Outputs after 1 epoch of training. The model produces faint blurry blobs
          that vaguely resemble digit shapes.
        </figcaption>
      </figure>
      <figure>
        <img src="media/b1_purenoise_epoch5.png" alt="Pure-noise denoising results after 5 epochs">
        <figcaption>
          Outputs after 5 epochs. The images look like an average over all digits:
          a thick, amorphous stroke in roughly digit-like locations.
          This matches the behavior of an MSE model that learns the per-pixel mean
          of the training distribution.
        </figcaption>
      </figure>
    </div>
    
    <p>
    Because the input noise is independent of the label, the optimal MSE solution
    is to predict the <em>dataset average</em> digit. The model cannot map specific
    noise patterns to specific digits, so it converges to a blurry “centroid”
    shape that minimizes the squared error over all training images.
    </p>
    
    <!-- ===================== B.2.1 Time conditioning ===================== -->
    
    <h3 id="part-b-2-1">Part B.2.1: Adding Time Conditioning to UNet</h3>
    
    <p>
    To enable iterative denoising, I modified the UNet to predict a flow field
    <u>u</u><sub>θ</sub>(x<sub>t</sub>, t) that depends on a continuous time
    parameter <code>t ∈ [0, 1]</code>. I introduced two fully-connected
    <code>FCBlock</code>s that embed the scalar time into channel-wise modulation
    vectors. These embeddings scale and shift intermediate feature maps in the
    bottleneck and decoder, letting the network respond differently at early
    (very noisy) versus late (almost clean) timesteps.
    </p>
    
    <p>
    The training objective is to match the true flow
    <code>u(x_t, t) = x₁ − x₀</code>, where
    <code>x₀ ~ N(0, I)</code>, <code>x₁</code> is a real MNIST digit, and
    <code>x_t = (1 − t) x₀ + t x₁</code> is their linear interpolation.
    </p>
    
    <!-- ===================== B.2.2 Training time-conditioned ===================== -->
    
    <h3 id="part-b-2-2">Part B.2.2: Training the Time-Conditioned UNet</h3>
    
    <p>
    I trained the time-conditioned UNet on MNIST with batch size 64 and hidden
    dimension D = 64. The optimizer is Adam with initial learning rate 1e−2 and an
    exponential learning rate scheduler that decays the LR by a factor of 0.1 over
    the full training run. For each batch, I sample a random timestep t, construct
    x<sub>t</sub> via linear interpolation between noise and data, and minimize the
    L2 difference between the predicted flow and the ground-truth flow
    x₁ − x₀.
    </p>
    
    <div class="image-row">
      <figure>
        <img src="media/b2_timecond_train_loss.png" alt="Training loss curve for time-conditioned flow-matching UNet">
        <figcaption>
          Training loss for the time-conditioned flow model. The loss steadily
          decreases and stabilizes, indicating that the UNet learns a consistent
          flow field from noise to data.
        </figcaption>
      </figure>
    </div>
    
    <!-- ===================== B.2.3 Sampling from time-conditioned ===================== -->
    
    <h3 id="part-b-2-3">Part B.2.3: Sampling from the Time-Conditioned UNet</h3>
    
    <p>
    To sample from the learned flow, I start from pure Gaussian noise
    x₀ ~ N(0, I) and integrate forward in time using Euler steps:
    </p>
    
    <p style="text-align:center;">
    <code>x_{t+Δt} = x_t + Δt · u_θ(x_t, t)</code>.
    </p>
    
    <p>
    I used T time steps with evenly spaced t values between 0 and 1. The figures
    below show samples after 1, 5, and 10 training epochs. Early on, the samples
    look like scrambled strokes, but as training progresses, more legible MNIST
    digits appear.
    </p>
    
    <div class="image-row">
      <figure>
        <img src="media/b2_timecond_samples_epoch1.png" alt="Time-conditioned sampling after epoch 1">
        <figcaption>
          Samples after 1 epoch. The model has learned a rough “white-on-black”
          prior but most digits are still highly distorted.
        </figcaption>
      </figure>
      <figure>
        <img src="media/b2_timecond_samples_epoch5.png" alt="Time-conditioned sampling after epoch 5">
        <figcaption>
          Samples after 5 epochs. Many digits are now recognizable, although strokes
          are still thick and occasionally merged.
        </figcaption>
      </figure>
      <figure>
        <img src="media/b2_timecond_samples_epoch10.png" alt="Time-conditioned sampling after epoch 10">
        <figcaption>
          Samples after 10 epochs. Most images resemble clear MNIST digits, though
          they are not as sharp as the ground-truth dataset.
        </figcaption>
      </figure>
    </div>
    
    <!-- ===================== B.2.4 Class conditioning ===================== -->
    
    <h3 id="part-b-2-4">Part B.2.4: Adding Class-Conditioning to UNet</h3>
    
    <p>
    To gain finer control over which digit is generated, I extended the
    time-conditioned UNet to also take a <strong>class-conditioning vector</strong>
    c for digits 0–9. The label is encoded as a 10-dimensional one-hot vector and
    fed through two additional <code>FCBlock</code>s. Their outputs modulate the
    same feature maps as the time embeddings:
    </p>
    
    <p style="text-align:center;">
    <code>unflatten' = c₁ ⊙ unflatten + t₁</code>,&nbsp;
    <code>up1' = c₂ ⊙ up1 + t₂</code>.
    </p>
    
    <p>
    During training, I randomly drop the class conditioning with probability
    p<sub>uncond</sub> = 0.1 by setting c to the zero vector. This “classifier-free
    guidance” trick lets the model learn both conditional and unconditional flows,
    which is crucial for guided sampling later.
    </p>
    
    <!-- ===================== B.2.5 Training class-conditioned ===================== -->
    
    <h3 id="part-b-2-5">Part B.2.5: Training the Class-Conditioned UNet</h3>
    
    <p>
    The class-conditioned UNet is trained with the same flow-matching objective as
    the time-only model, but now also receives a digit label. I keep the batch
    size at 64, use Adam with initial learning rate 1e−2, and again apply an
    exponential LR scheduler that decays the LR by 0.1 over 10 epochs. The model
    learns to map from interpolated states x<sub>t</sub> to flows that depend on
    both time t and class c.
    </p>
    
    <div class="image-row">
      <figure>
        <img src="media/b2_classcond_train_loss.png" alt="Training loss curve for class-conditioned UNet">
        <figcaption>
          Training loss for the class-conditioned UNet. The behavior is similar to
          the time-only model, showing smooth convergence as the network learns the
          joint time-and-class-conditioned flow.
        </figcaption>
      </figure>
    </div>
    
    <!-- ===================== B.2.6 Sampling from class-conditioned ===================== -->
    
    <h3 id="part-b-2-6">Part B.2.6: Sampling from the Class-Conditioned UNet</h3>
    
    <p>
    For sampling, I use classifier-free guidance with scale γ = 5. At each
    timestep, I compute both an unconditional flow
    <code>u_uncond = u_θ(x_t, t, c=Ø)</code> and a conditional flow
    <code>u_cond = u_θ(x_t, t, c)</code>, then combine them as
    </p>
    
    <p style="text-align:center;">
    <code>u = u_uncond + γ · (u_cond − u_uncond)</code>.
    </p>
    
    <p>
    This pushes samples toward images that are strongly aligned with the requested
    digit class while still staying on the learned data manifold.
    </p>
    
    <h4>With Learning Rate Scheduler</h4>
    
    <p>
    Here I use the exponential LR scheduler described above. I generate 4 samples
    for each digit 0–9, arranged as 4 rows by 10 columns, after 1, 5, and 10
    epochs of training.
    </p>
    
    <div class="image-row">
      <figure>
        <img src="media/b2_classcond_sched_epoch1.png" alt="Class-conditioned samples with scheduler after epoch 1">
        <figcaption>
          Class-conditioned samples after 1 epoch (with LR scheduler). The correct
          digit structure is already visible, but many strokes are noisy or broken.
        </figcaption>
      </figure>
    </div>
    
    <div class="image-row">
      <figure>
        <img src="media/b2_classcond_sched_epoch5.png" alt="Class-conditioned samples with scheduler after epoch 5">
        <figcaption>
          Samples after 5 epochs. The digits are much clearer and more consistently
          match their target classes, with cleaner backgrounds.
        </figcaption>
      </figure>
    </div>
    
    <div class="image-row">
      <figure>
        <img src="media/b2_classcond_sched_epoch10.png" alt="Class-conditioned samples with scheduler after epoch 10">
        <figcaption>
          Samples after 10 epochs. The model produces sharp, well-separated digits
          that closely resemble MNIST, demonstrating the benefits of class
          conditioning and guidance.
        </figcaption>
      </figure>
    </div>
    
    <h4>Removing the Learning Rate Scheduler</h4>
    
    <p>
    To simplify training, I also experimented with removing the LR scheduler and
    using a smaller constant learning rate (5e−3) instead. The rest of the setup
    remains the same. I again visualize 4 samples per digit after 1, 5, and 10
    epochs.
    </p>
    
    <div class="image-row">
      <figure>
        <img src="media/b2_classcond_nosched_epoch1.png" alt="Class-conditioned samples without scheduler after epoch 1">
        <figcaption>
          Samples after 1 epoch without an LR scheduler. The digits are slightly
          noisier than in the scheduled case but still mostly recognizable.
        </figcaption>
      </figure>
    </div>
    
    <div class="image-row">
      <figure>
        <img src="media/b2_classcond_nosched_epoch5.png" alt="Class-conditioned samples without scheduler after epoch 5">
        <figcaption>
          Samples after 5 epochs. The model catches up to the scheduled version:
          digit identities are clear and backgrounds are mostly clean.
        </figcaption>
      </figure>
    </div>
    
    <div class="image-row">
      <figure>
        <img src="media/b2_classcond_nosched_epoch10.png" alt="Class-conditioned samples without scheduler after epoch 10">
        <figcaption>
          Samples after 10 epochs. Using a smaller constant learning rate achieves
          comparable sample quality to the exponential scheduler while keeping the
          training loop simpler.
        </figcaption>
      </figure>
    </div>
    
    <p>
    Overall, class conditioning and classifier-free guidance dramatically improve
    sample quality and controllability. The learning rate scheduler helps with
    stability early in training, but a carefully chosen fixed learning rate can
    reach very similar performance.
    </p>


  </main>

  <footer>
    © <script>document.write(new Date().getFullYear());</script> Project 5. All images and results by the author.
  </footer>
</body>
</html>

